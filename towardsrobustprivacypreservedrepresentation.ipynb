{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7189395,"sourceType":"datasetVersion","datasetId":4156759},{"sourceId":7288363,"sourceType":"datasetVersion","datasetId":4226698},{"sourceId":7367272,"sourceType":"datasetVersion","datasetId":4152755}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"prefix = '/kaggle/input/robust-and-privacy-preserving-text-representations/Robust_and_Privacy_preserving_Text_Representations-master/Robust_and_Privacy_preserving_Text_Representations-master/'\nprefix2 = '/kaggle/working/'\nraw_dataset_path = prefix + 'dataset/TrustPilot/'\ndataset_path = prefix + 'dataset/WWW2015_processed/'\nfold_path = dataset_path + 'StratifiedFold/'\nprefix3 = '/kaggle/input/resultsrobustprivacy/'\nresult_path = prefix3\ntrain_csv = prefix + 'dataset/WWW2015_processed/train.csv'\ntest_csv = prefix + 'dataset/WWW2015_processed/test.csv'\nvalid_csv = prefix + 'dataset/WWW2015_processed/valid.csv'\ntotal_csv = prefix + 'dataset/WWW2015_processed/total.csv'\ntrain_csv_modified = prefix + 'dataset/WWW2015_processed/train_modified.csv'\ntest_csv_modified = prefix + 'dataset/WWW2015_processed/test_modified.csv'\nvalid_csv_modified = prefix + 'dataset/WWW2015_processed/valid_modified.csv'\ntotal_csv_modified = prefix + 'dataset/WWW2015_processed/total_modified.csv'\n\ntrain_fold_1 = prefix + 'dataset/WWW2015_processed/train_fold_1.csv'\ntest_fold_1 = prefix + 'dataset/WWW2015_processed/test_fold_1.csv'\n\ntrain_fold_2 = prefix + 'dataset/WWW2015_processed/train_fold_2.csv'\ntest_fold_2 = prefix + 'dataset/WWW2015_processed/test_fold_2.csv'\n\ntrain_fold_3 = prefix + 'dataset/WWW2015_processed/train_fold_3.csv'\ntest_fold_3 = prefix + 'dataset/WWW2015_processed/test_fold_3.csv'\n\ntrain_fold_4 = prefix + 'dataset/WWW2015_processed/train_fold_4.csv'\ntest_fold_4 = prefix + 'dataset/WWW2015_processed/test_fold_4.csv'\n\ntrain_fold_5 = prefix + 'dataset/WWW2015_processed/train_fold_5.csv'\ntest_fold_5 = prefix + 'dataset/WWW2015_processed/test_fold_5.csv'\n\ntrain_fold_1_acc_per_epoch = prefix2 + 'train_fold_1_acc_per_epoch.csv'\ntrain_fold_1_weighted_f1_per_epoch = prefix2 + 'train_fold_1_weighted_f1_per_epoch.csv'\nval_fold_1_acc_per_epoch = prefix2 + 'val_fold_1_acc_per_epoch.csv'\nval_fold_1_weighted_f1_per_epoch = prefix2 + 'val_fold_1_weighted_f1_per_epoch.csv'\n\ntrain_fold_2_acc_per_epoch = prefix2 + 'train_fold_2_acc_per_epoch.csv'\ntrain_fold_2_weighted_f1_per_epoch = prefix2 + 'train_fold_2_weighted_f1_per_epoch.csv'\nval_fold_2_acc_per_epoch = prefix2 + 'val_fold_2_acc_per_epoch.csv'\nval_fold_2_weighted_f1_per_epoch = prefix2 + 'val_fold_2_weighted_f1_per_epoch.csv'\n\ntrain_fold_3_acc_per_epoch = prefix2 + 'train_fold_3_acc_per_epoch.csv'\ntrain_fold_3_weighted_f1_per_epoch = prefix2 + 'train_fold_3_weighted_f1_per_epoch.csv'\nval_fold_3_acc_per_epoch = prefix2 + 'val_fold_3_acc_per_epoch.csv'\nval_fold_3_weighted_f1_per_epoch = prefix2 + 'val_fold_3_weighted_f1_per_epoch.csv'\n\ntrain_fold_4_acc_per_epoch = prefix2 + 'train_fold_4_acc_per_epoch.csv'\ntrain_fold_4_weighted_f1_per_epoch = prefix2 + 'train_fold_4_weighted_f1_per_epoch.csv'\nval_fold_4_acc_per_epoch = prefix2 + 'val_fold_4_acc_per_epoch.csv'\nval_fold_4_weighted_f1_per_epoch = prefix2 + 'val_fold_4_weighted_f1_per_epoch.csv'\n\ntrain_fold_5_acc_per_epoch = prefix + 'train_fold_5_acc_per_epoch.csv'\ntrain_fold_5_weighted_f1_per_epoch = prefix + 'train_fold_5_weighted_f1_per_epoch.csv'\nval_fold_5_acc_per_epoch = prefix + 'val_fold_5_acc_per_epoch.csv'\nval_fold_5_weighted_f1_per_epoch = prefix + 'val_fold_5_weighted_f1_per_epoch.csv'\n\ntrain_fold_1_gender_acc_per_epoch = prefix2 + 'train_fold_1_gender_acc_per_epoch.csv'\ntrain_fold_1_gender_weighted_f1_per_epoch = prefix2 + 'train_fold_1_gender_weighted_f1_per_epoch.csv'\nval_fold_1_gender_acc_per_epoch = prefix2 + 'val_fold_1_gender_acc_per_epoch.csv'\nval_fold_1_gender_weighted_f1_per_epoch = prefix2 + 'val_fold_1_gender_weighted_f1_per_epoch.csv'\n\ntrain_fold_2_gender_acc_per_epoch = prefix2 + 'train_fold_2_gender_acc_per_epoch.csv'\ntrain_fold_2_gender_weighted_f1_per_epoch = prefix2 + 'train_fold_2_gender_weighted_f1_per_epoch.csv'\nval_fold_2_gender_acc_per_epoch = prefix2 + 'val_fold_2_gender_acc_per_epoch.csv'\nval_fold_2_gender_weighted_f1_per_epoch = prefix2 + 'val_fold_2_gender_weighted_f1_per_epoch.csv'\n\ntrain_fold_3_gender_acc_per_epoch = prefix2 + 'train_fold_3_gender_acc_per_epoch.csv'\ntrain_fold_3_gender_weighted_f1_per_epoch = prefix2 + 'train_fold_3_gender_weighted_f1_per_epoch.csv'\nval_fold_3_gender_acc_per_epoch = prefix2 + 'val_fold_3_gender_acc_per_epoch.csv'\nval_fold_3_gender_weighted_f1_per_epoch = prefix2 + 'val_fold_3_gender_weighted_f1_per_epoch.csv'\n\ntrain_fold_4_gender_acc_per_epoch = prefix2 + 'train_fold_4_gender_acc_per_epoch.csv'\ntrain_fold_4_gender_weighted_f1_per_epoch = prefix2 + 'train_fold_4_gender_weighted_f1_per_epoch.csv'\nval_fold_4_gender_acc_per_epoch = prefix2 + 'val_fold_4_gender_acc_per_epoch.csv'\nval_fold_4_gender_weighted_f1_per_epoch = prefix2 + 'val_fold_4_gender_weighted_f1_per_epoch.csv'\n\ntrain_fold_5_gender_acc_per_epoch = prefix + 'train_fold_5_gender_acc_per_epoch.csv'\ntrain_fold_5_gender_weighted_f1_per_epoch = prefix + 'train_fold_5_gender_weighted_f1_per_epoch.csv'\nval_fold_5_gender_acc_per_epoch = prefix + 'val_fold_5_gender_acc_per_epoch.csv'\nval_fold_5_gender_weighted_f1_per_epoch = prefix + 'val_fold_5_gender_weighted_f1_per_epoch.csv'\n\nprefix3 = '/kaggle/input/resultsrobustprivacy/'\ntrain_fold_1_acc_per_epoch_results = prefix3 + 'train_fold_1_acc_per_epoch.csv'\ntrain_fold_1_weighted_f1_per_epoch_results = prefix3 + 'train_fold_1_weighted_f1_per_epoch.csv'\nval_fold_1_acc_per_epoch_results = prefix3 + 'val_fold_1_acc_per_epoch.csv'\nval_fold_1_weighted_f1_per_epoch_results = prefix3 + 'val_fold_1_weighted_f1_per_epoch.csv'\n\ntrain_fold_2_acc_per_epoch_results = prefix3 + 'train_fold_2_acc_per_epoch.csv'\ntrain_fold_2_weighted_f1_per_epoch_results = prefix3 + 'train_fold_2_weighted_f1_per_epoch.csv'\nval_fold_2_acc_per_epoch_results = prefix3 + 'val_fold_2_acc_per_epoch.csv'\nval_fold_2_weighted_f1_per_epoch_results = prefix3 + 'val_fold_2_weighted_f1_per_epoch.csv'\n\ntrain_fold_3_acc_per_epoch_results = prefix3 + 'train_fold_3_acc_per_epoch.csv'\ntrain_fold_3_weighted_f1_per_epoch_results = prefix3 + 'train_fold_3_weighted_f1_per_epoch.csv'\nval_fold_3_acc_per_epoch_results = prefix3 + 'val_fold_3_acc_per_epoch.csv'\nval_fold_3_weighted_f1_per_epoch_results = prefix3 + 'val_fold_3_weighted_f1_per_epoch.csv'\n\ntrain_fold_4_acc_per_epoch_results = prefix3 + 'train_fold_4_acc_per_epoch.csv'\ntrain_fold_4_weighted_f1_per_epoch_results = prefix3 + 'train_fold_4_weighted_f1_per_epoch.csv'\nval_fold_4_acc_per_epoch_results = prefix3 + 'val_fold_4_acc_per_epoch.csv'\nval_fold_4_weighted_f1_per_epoch_results = prefix3 + 'val_fold_4_weighted_f1_per_epoch.csv'\n\ntrain_fold_5_acc_per_epoch_results = prefix3 + 'train_fold_5_acc_per_epoch.csv'\ntrain_fold_5_weighted_f1_per_epoch_results = prefix3 + 'train_fold_5_weighted_f1_per_epoch.csv'\nval_fold_5_acc_per_epoch_results = prefix3 + 'val_fold_5_acc_per_epoch.csv'\nval_fold_5_weighted_f1_per_epoch_results = prefix3 + 'val_fold_5_weighted_f1_per_epoch.csv'\n\ntextCNNModelPath = '/kaggle/input/robust-and-privacy-preserving-text-representations/TextCNNmodel_epoch9.pt'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-09T05:57:01.194539Z","iopub.execute_input":"2024-01-09T05:57:01.195278Z","iopub.status.idle":"2024-01-09T05:57:01.209600Z","shell.execute_reply.started":"2024-01-09T05:57:01.195244Z","shell.execute_reply":"2024-01-09T05:57:01.208679Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_fold_1_acc_per_epoch = result_path + 'train_fold_1_acc_per_epoch.csv'\ntrain_fold_1_weighted_f1_per_epoch = result_path + 'train_fold_1_weighted_f1_per_epoch.csv'\nval_fold_1_acc_per_epoch = result_path + 'val_fold_1_acc_per_epoch.csv'\nval_fold_1_weighted_f1_per_epoch = result_path + 'val_fold_1_weighted_f1_per_epoch.csv'\n\ntrain_fold_2_acc_per_epoch = result_path + 'train_fold_2_acc_per_epoch.csv'\ntrain_fold_2_weighted_f1_per_epoch = result_path + 'train_fold_2_weighted_f1_per_epoch.csv'\nval_fold_2_acc_per_epoch = result_path + 'val_fold_2_acc_per_epoch.csv'\nval_fold_2_weighted_f1_per_epoch = result_path + 'val_fold_2_weighted_f1_per_epoch.csv'\n\ntrain_fold_3_acc_per_epoch = result_path + 'train_fold_3_acc_per_epoch.csv'\ntrain_fold_3_weighted_f1_per_epoch = result_path + 'train_fold_3_weighted_f1_per_epoch.csv'\nval_fold_3_acc_per_epoch = result_path + 'val_fold_3_acc_per_epoch.csv'\nval_fold_3_weighted_f1_per_epoch = result_path + 'val_fold_3_weighted_f1_per_epoch.csv'\n\ntrain_fold_4_acc_per_epoch = result_path + 'train_fold_4_acc_per_epoch.csv'\ntrain_fold_4_weighted_f1_per_epoch = result_path + 'train_fold_4_weighted_f1_per_epoch.csv'\nval_fold_4_acc_per_epoch = result_path + 'val_fold_4_acc_per_epoch.csv'\nval_fold_4_weighted_f1_per_epoch = result_path + 'val_fold_4_weighted_f1_per_epoch.csv'\n\ntrain_fold_5_acc_per_epoch = result_path + 'train_fold_5_acc_per_epoch.csv'\ntrain_fold_5_weighted_f1_per_epoch = result_path + 'train_fold_5_weighted_f1_per_epoch.csv'\nval_fold_5_acc_per_epoch = result_path + 'val_fold_5_acc_per_epoch.csv'\nval_fold_5_weighted_f1_per_epoch = result_path + 'val_fold_5_weighted_f1_per_epoch.csv'","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:45:27.381564Z","iopub.execute_input":"2024-01-08T17:45:27.382083Z","iopub.status.idle":"2024-01-08T17:45:27.389541Z","shell.execute_reply.started":"2024-01-08T17:45:27.382048Z","shell.execute_reply":"2024-01-08T17:45:27.388353Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Read the ACCURACY AND WEIGHTED F1 PER EPOCH OF EACH FOLD.\n#Each file has the individual accuracy value for the 20 epochs\n\nimport pandas as pd\nvf1acc = pd.read_csv(val_fold_1_acc_per_epoch, header = None).to_numpy()\nvf2acc = pd.read_csv(val_fold_2_acc_per_epoch, header = None).to_numpy()\nvf3acc = pd.read_csv(val_fold_3_acc_per_epoch, header = None).to_numpy()\nvf4acc = pd.read_csv(val_fold_4_acc_per_epoch, header = None).to_numpy()\nvf5acc = pd.read_csv(val_fold_5_acc_per_epoch, header = None).to_numpy()\n\nvf1f1 = pd.read_csv(val_fold_1_weighted_f1_per_epoch, header = None).to_numpy()\nvf2f1 = pd.read_csv(val_fold_2_weighted_f1_per_epoch, header = None).to_numpy()\nvf3f1 = pd.read_csv(val_fold_3_weighted_f1_per_epoch, header = None).to_numpy()\nvf4f1 = pd.read_csv(val_fold_4_weighted_f1_per_epoch, header = None).to_numpy()\nvf5f1 = pd.read_csv(val_fold_5_weighted_f1_per_epoch, header = None).to_numpy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AVG THE RESULTS FOR EACH EPOCH ACROSS THE 5 FOLDS\nimport numpy as np\nepoch_acc_avg_for_all_folds = np.zeros((20))\nepoch_f1_avg_for_all_folds = np.zeros((20))\nfor i in range(20):\n    epoch_acc_avg_for_all_folds[i] = (vf1acc[i] + vf2acc[i] + vf3acc[i] + vf4acc[i] +vf5acc[i])/5\n    epoch_f1_avg_for_all_folds[i] = (vf1f1[i] + vf2f1[i] + vf3f1[i] + vf4f1[i] +vf5f1[i])/5\n    \n\n                           ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchtext==0.6.0","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:49:56.124544Z","iopub.execute_input":"2024-01-09T05:49:56.124897Z","iopub.status.idle":"2024-01-09T05:50:10.214285Z","shell.execute_reply.started":"2024-01-09T05:49:56.124864Z","shell.execute_reply":"2024-01-09T05:50:10.213093Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchtext==0.6.0\n  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m668.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (4.66.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (2.31.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (1.24.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (1.16.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from torchtext==0.6.0) (0.1.99)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.6.0) (2023.11.17)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchtext==0.6.0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\nInstalling collected packages: torchtext\n  Attempting uninstall: torchtext\n    Found existing installation: torchtext 0.15.1\n    Uninstalling torchtext-0.15.1:\n      Successfully uninstalled torchtext-0.15.1\nSuccessfully installed torchtext-0.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchtext\ntorchtext.__version__","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:56:47.454736Z","iopub.execute_input":"2024-01-09T05:56:47.455501Z","iopub.status.idle":"2024-01-09T05:56:50.946670Z","shell.execute_reply.started":"2024-01-09T05:56:47.455469Z","shell.execute_reply":"2024-01-09T05:56:50.945659Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'0.6.0'"},"metadata":{}}]},{"cell_type":"code","source":"Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchtext\nfrom torchtext.data import get_tokenizer\nfrom torchtext import data, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport spacy\nspacy.load(\"en_core_web_sm\")\n\nimport random\n\nSEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:57:15.216859Z","iopub.execute_input":"2024-01-09T05:57:15.217238Z","iopub.status.idle":"2024-01-09T05:57:20.160462Z","shell.execute_reply.started":"2024-01-09T05:57:15.217207Z","shell.execute_reply":"2024-01-09T05:57:20.159661Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nTEXT = data.Field('spacy')\nRATING_LABEL = data.LabelField()\nGENDER_LABEL = data.LabelField()\nAGE_LABEL = data.LabelField()\nLOCATION_LABEL = data.LabelField()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:57:26.886035Z","iopub.execute_input":"2024-01-09T05:57:26.887064Z","iopub.status.idle":"2024-01-09T05:57:26.892034Z","shell.execute_reply.started":"2024-01-09T05:57:26.887028Z","shell.execute_reply":"2024-01-09T05:57:26.891129Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Hyperparameters\n\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nEMBEDDING_DIM = 100\nN_FILTERS = 128\nFILTER_SIZES = [3, 4, 5]\nOUTPUT_DIM = 5\nDROPOUT = 0.2\nNUM_EPOCHS = 20\n\nLAMBDA = 1e-3\nHIDDEN_DIM = 300","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:57:32.719960Z","iopub.execute_input":"2024-01-09T05:57:32.720304Z","iopub.status.idle":"2024-01-09T05:57:32.725657Z","shell.execute_reply.started":"2024-01-09T05:57:32.720271Z","shell.execute_reply":"2024-01-09T05:57:32.724603Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"i=3\ntrain_data, valid_data, test_data = data.TabularDataset.splits(path=fold_path,\n                                                               train=\"train_fold_{}.csv\".format(i),\n                                                               validation=\"test_fold_{}.csv\".format(i),\n                                                               test=\"test_fold_{}.csv\".format(i),\n                                                               fields=[('text', TEXT), ('rating', RATING_LABEL), ('gender', GENDER_LABEL),\n                                                                       ('age', AGE_LABEL), ('location', LOCATION_LABEL)],\n                                                               format=\"csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=BATCH_SIZE,\n                                                               device=device,\n                                                               sort_key=lambda x: len(x.text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\nRATING_LABEL.build_vocab(train_data)\nGENDER_LABEL.build_vocab(train_data)\nAGE_LABEL.build_vocab(train_data)\nLOCATION_LABEL.build_vocab(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextCNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n                 dropout):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        self.conv_0 = nn.Conv2d(in_channels=1,\n                                out_channels=n_filters,\n                                kernel_size=(filter_sizes[0], embedding_dim))\n\n        self.conv_1 = nn.Conv2d(in_channels=1,\n                                out_channels=n_filters,\n                                kernel_size=(filter_sizes[1], embedding_dim))\n\n        self.conv_2 = nn.Conv2d(in_channels=1,\n                                out_channels=n_filters,\n                                kernel_size=(filter_sizes[2], embedding_dim))\n\n        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text):\n        # text = [sent len, batch size]\n        #print(\"CNN First, text.size() = \", text.size())\n        text = text.permute(1, 0)\n        #print(\"CNN Second, text.size() = \", text.size())\n        # text = [batch size, sent len]\n\n        embedded = self.embedding(text)\n        #print(\"CNN Third, embedded.size() = \", embedded.size())\n        # embedded = [batch size, sent len, emb dim]\n\n        embedded = embedded.unsqueeze(1)\n        #print(\"CNN Fourth, embedded.size() = \", embedded.size())\n\n        # embedded = [batch size, 1, sent len, emb dim]\n\n        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n        #print(\"CNN Fifth, conved_2.size() = \", conved_2.size())\n        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n\n        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n        #print(\"CNN Sixth, pooled_2.size() = \", pooled_2.size())\n\n        # torch.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n        # pooled_n = [batch size, n_filters]\n\n        #****** this cat layer is equivalent to h_drop in the tensorflow implementation\n        # this cat layer is the input to the private attribute discriminators\n\n        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n\n        #print(\"CNN Seventh, cat.size() = \", cat.size())\n        ##### cat = [batch size, n_filters * len(filter_sizes)]******#\n\n\n        #print(\"CNN Eighth, self.fc(cat).size() = \", self.fc(cat).size())\n        return self.fc(cat)\n\n    # I added this from the HanXundong Postag implementation and replaced it with the correct Sentiment hidden state.\n    def hidden_state(self, text):\n\n       # text = [sent len, batch size]\n        #print(\"CNN First, text.size() = \", text.size())\n        text = text.permute(1, 0)\n        #print(\"CNN Second, text.size() = \", text.size())\n        # text = [batch size, sent len]\n\n        embedded = self.embedding(text)\n        #print(\"CNN Third, embedded.size() = \", embedded.size())\n        # embedded = [batch size, sent len, emb dim]\n\n        embedded = embedded.unsqueeze(1)\n        #print(\"CNN Fourth, embedded.size() = \", embedded.size())\n\n        # embedded = [batch size, 1, sent len, emb dim]\n\n        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n        #print(\"CNN Fifth, conved_2.size() = \", conved_2.size())\n        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n\n        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n        #print(\"CNN Sixth, pooled_2.size() = \", pooled_2.size())\n\n        # torch.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n        # pooled_n = [batch size, n_filters]\n\n        #****** this cat layer is equivalent to h_drop in the tensorflow implementation\n        # this cat layer is the input to the private attribute discriminators\n\n        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n\n        return cat","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:57:52.479983Z","iopub.execute_input":"2024-01-09T05:57:52.480350Z","iopub.status.idle":"2024-01-09T05:57:52.496271Z","shell.execute_reply.started":"2024-01-09T05:57:52.480319Z","shell.execute_reply":"2024-01-09T05:57:52.495368Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Training and evaluating the Text CNN model for rating classification baseline result\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torchmetrics import F1Score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn import metrics\n\n\n#FOR LOOP TO ITERATE FOR EACH FOLDS\nfor j in range(1, 6):\n    \n    #GET THE DATA\n    train_data, valid_data, test_data = data.TabularDataset.splits(path=fold_path,\n                                                                   train=\"train_fold_{}.csv\".format(j),\n                                                                   validation=\"test_fold_{}.csv\".format(j),\n                                                                   test=\"test_fold_{}.csv\".format(j),\n                                                                   fields=[('text', TEXT), ('rating', RATING_LABEL), ('gender', GENDER_LABEL),\n                                                                           ('age', AGE_LABEL), ('location', LOCATION_LABEL)],\n                                                                   format=\"csv\")\n    \n    #GET THE ITERATOR\n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=BATCH_SIZE,\n                                                               device=device,\n                                                               sort_key=lambda x: len(x.text))\n    \n    #BUILD VOCAB\n    TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n    RATING_LABEL.build_vocab(train_data)\n    GENDER_LABEL.build_vocab(train_data)\n    AGE_LABEL.build_vocab(train_data)\n    LOCATION_LABEL.build_vocab(train_data)\n    \n    INPUT_DIM = len(TEXT.vocab)\n\n    # Create an instance of the TextCNN model\n    model = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n\n    #model.apply(init_weights)\n    # USE PRETRAINED GLOVE EMBEDDINGS \n    pretrained_embeddings = TEXT.vocab.vectors\n    model.embedding.weight.data.copy_(pretrained_embeddings) \n    \n    #DEFINE THE LOSS FUNCTION AND OPTIMIZER\n    #class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y.numpy())\n    sc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\n    criterion = nn.CrossEntropyLoss(weight = sc).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    model = model.to(device)\n    \n    ########## Train and Validation ##########\n    ## Add other evaluation metrics especially F1 score and confusion matrix.\n    #Implement 5 fold cross validation\n    #### First train the text classification model and save the model with the best validation loss.\n    ## Make sure you storing the right loss values.\n   \n    #DEFINE MULTICLASS F1 EVALUATION METRIC\n    f1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\n    total_step = len(train_iter)\n\n    best_valid_loss = float('inf')\n    best_epoch = -1\n\n    #saved_model = model_name.format(SAMPLING_INDEX+1)\n    \n    \n    #LOSS_PER_EPOCH\n    train_loss_per_epoch = []\n    valid_loss_per_epoch = []\n    \n    #ACCURACY PER EPOCH\n    train_acc_per_epoch = []\n    valid_acc_per_epoch = []\n    \n    #WEIGHTED F1 PER EPOCH\n    train_weighted_F1_per_epoch = []\n    val_weighted_F1_per_epoch = []\n    \n    ########################### TRAINING STARTS #########################################\n    \n    for epoch in range(NUM_EPOCHS):\n        \n        model.train()\n        \n        #LIST TO STORE THE LOSS OF ENTIRE EPOCH BY APPENDING LOSS OF EACH BATCH\n        total_loss = []\n        train_total_correct = 0\n        \n        #STORE THE GROUND TRUTH AND PREDICTION OUTPUT OF THE TEXTCNN MODEL FOR THE ENTIRE EPOCH\n        \n        #TRAINING GROUND TRUTH AND PREDICTION\n        y_tot = torch.empty((0))\n        pred_tot = torch.empty((0))\n        \n        #VALID GROUND TRUTH AND PREDICTION\n        y_valid_tot = torch.empty((0))\n        pred_valid_tot = torch.empty((0))\n        \n        ########################## BATCH TRAINING STARTS #############################\n        \n        for i, batch in enumerate(train_iter):\n            \n            #GET INPUT FOR A BATCH\n            text = batch.text\n            y = batch.rating\n\n            # Forward pass\n            # y_pred = model(text).squeeze(1).float()\n            \n            #GET TEXT CNN OUTPUT. GIVES THE PREDICTION IN POBABILITIES\n            \n            y_pred = model(text).squeeze(1)\n\n            #GET THE LOSS. DOUBLE CHECK IF THIS LOSS WORKS WITH\n            #PROBABILITY OUTPUT OR DISCRETE\n            #loss for entire batch size\n            \n            ####### IMP QSSSSSS???????##########################\n            \n            ## WHY is y USED FOR CRITERION LOSS AS WELL AS TRAIN_TOTAL_CORRECT CALCULATION???\n            ## y CAN BE PROBABILITY OR ONE HOT ENCODED, SO FIND OUT WHAT IT IS????\n            ### THIS MIGHT BE THE REASON WHY THE LOSS AND ACCURACY DOESN'T MATCH !!\n            ###########################################\n            \n            loss = criterion(y_pred, y)\n            \n            #CHANGE PROBABILITIY BASED OUTPUT TO DISCRETE OUTPUT\n            pred = torch.argmax(y_pred.data, dim=1)\n\n            #ADD NUMBER OF CORRECT PREDICTIONS IN THE CURRENT BATCH TO \n            #THE TOTAL NUMBER OF CORRECT PREDICTIONS OF THE MODEL\n            \n            train_total_correct += (pred == y).sum().item()\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # len(total_loss) = len(train_data)/batch_size\n            #APPEND LOSS FOR EACH BATCH DATA TO LIST\n            \n            total_loss.append(loss.item())\n\n            #PRINT - EPOCH, TOTAL_EPOCHS, BATCH_STEP IN THE EPOCH, BATCH_LOSS\n            if (i + 1) % 10 == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                      .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n            \n            #APPEND BATCH GROUNT TRUTH TO OVERALL GROUND TRUTH\n            y_tot = torch.cat((y_tot, y.cpu()))\n            \n            #APPEND BATCH TEXT CNN OUTPUT TO OVERALL TEXT CNN OUTPUT\n            pred_tot = torch.cat((pred_tot, pred.cpu()))\n            \n        ########################## BATCH TRAINING ENDS #################################\n        \n        \n        ####################### STORE THE RESULTS ######################################\n        \n        #every element is the loss for the batch. \n        #SUM ALL THE BATCH LOSSES FOR A EPOCH AND \n        #DIVIDE BY THE TOTAL BATCH LOSSES IN EPOCH TO STORE AVG LOSS FOR THE CURRENT EPOCH\n        \n        train_loss_per_epoch.append(sum(total_loss)/len(total_loss))\n        \n        # STORE ACCURACY FOR THE CURRENT EPOCH\n        train_acc_per_epoch.append(100 * train_total_correct / len(train_data))\n        \n        #STORE WEIGHTED FOR FOR THE CURRENT EPOCH\n        train_weighted_F1_per_epoch.append(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        # STORE CONFUSION MATRIX OF CURRENT EPOCH\n        conf_mat = confusion_matrix(y_tot.cpu(), pred_tot.cpu())\n        \n        ################# PRINT RESULTS FOR CURRENT EPOCH #############################\n        \n        #PRINT ALL BATCH LOSSES AND AVG FOR THE CURRENT EPOCH\n        print(\"Training total_loss = {}\".format(total_loss))\n        print(\"Training avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss)/len(total_loss)))\n\n        #PRINT ACCURACY FOR EPOCH AND F1 FOR THE CURRENT EPOCH\n        print(\"Training total_accuracy = {:.4f}%\".format(100 * train_total_correct / len(train_data)))\n        print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_tot.cpu(), y_tot.cpu())))\n\n        #PRINT CONFUSION MATRIX\n        print(conf_mat)\n        \n        #PRINT METRICS CLASSIFICATION REPORT\n        print(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), digits=3))\n\n        ####################### TRAINING ENDS ##########################################\n        \n        \n        ####################### VALIDATION STARTS ######################################\n        \n        model.eval()\n        \n        #STORE TOTAL VALIDATION \n        total_valid_correct = 0\n        \n        # CALCULATE VALIDATION LOSS\n        valid_loss = 0.0\n        total_loss_valid =[]\n        \n        ############################ BATCH VALIDATION STARTS ###############################\n        \n        for i, batch in enumerate(valid_iter):\n\n            \n            # GET THE INPUT TEXT AND RATING\n            text = batch.text\n            y_valid = batch.rating\n\n\n\n            #print(text)\n            #print(y_valid)\n            \n            # IF TEXT SIZE IS LESS THAN 5, THEN SKIP THAT DATA POINT FOR COMPUTATIONAL PURPOSES\n            if text.size()[0] < 5:\n              continue\n            \n            \n            #### QUESTION - IF y_valid is probability based or discrete. \n            # WHY IS y_valid used for loss calculation as well as total_valid_Correct??\n            # GET MODEL PREDICTION ON BATCH VALIDATION DATA\n            y_pred_valid = model(text).squeeze(1)\n            pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n            \n            #CALCULATE VALIDATION LOSS\n            v_loss = criterion(y_pred_valid, y_valid)\n            \n            \n\n            # CALCULATE NUMBER OF CORRECT PREDICTIONS\n            total_valid_correct += (pred_valid == y_valid).sum().item()\n            \n            #ADD LOSS FOR EACH BATCH TO VALID LOSS AND LIST\n            valid_loss += v_loss.item()\n            total_loss_valid.append(v_loss.item())\n            \n            #ADD BATCH VALIDATION GROUND TRUTH TO ENTIRE DATA\n            y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n            pred_valid_tot = torch.cat((pred_valid_tot, pred_valid.cpu()))\n\n        if valid_loss < best_valid_loss:\n                best_valid_loss = valid_loss\n                best_epoch = epoch\n        #        torch.save(model.state_dict(), saved_model)\n\n        #if epoch == 9:\n            #torch.save(model.state_dict(), 'TextCNNmodel_epoch9.pt')\n\n\n        ########################### END OF BATCH VALIDATION ##############################\n        \n        \n        ########################### STORE THE RESULTS #####################################\n\n        valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))\n        valid_acc_per_epoch.append(100 * total_valid_correct / len(valid_data))\n        val_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        avg_loss = valid_loss * BATCH_SIZE/ len(valid_data)\n        conf_mat = confusion_matrix(y_valid_tot.cpu(), pred_valid_tot.cpu())\n        \n        ########################### END STORE RESULTS #####################################\n        \n        ########################### PRINT RESULTS #########################################\n        \n        print(\"Valid total_loss = {}\".format(total_loss_valid))\n        print(\"Valid avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss_valid)/len(total_loss_valid)))\n\n        # this is wrong because valid loss sums up the total loss for a batch and thus should be divided by len(valid_data)/batch_size\n        \n        print(\"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}, F1: {:.4f}%\\n\".format(avg_loss, 100 * total_valid_correct / len(valid_data), f1_rating(pred_valid_tot.cpu(), y_valid_tot.cpu())))\n        \n        print(conf_mat)\n\n        print(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), digits=3))\n        \n        ############################ END PRINT RESULTS #####################################\n    \n    ############################ END VALIDATION AND EPOCH TRAINING #########################\n    \n    ############################ STORE CURRENT FOLD RESULTS ################################\n    \n    #CONVERT TO DATAFRAME AND \n    #DOWNLOAD AS CSV - TRAIN RATING ACCURACY FILE, TRAIN RATING F1,\n    #VAL RATING ACCURACY AND VAL RATING F1\n    #CONSISTING OF ALL 20 EPOCHS RESULT FOR CURRENT FOLD\n    \n    #RATING CLASSIFIER ACCURACY\n    train_acc_per_epoch_df = pd.DataFrame(train_acc_per_epoch)\n    train_acc_per_epoch_df.to_csv(\"train_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n        \n    #RATING CLASSIFIER WEIGHTED F1\n    train_weighted_F1_per_epoch_df = pd.DataFrame(train_weighted_F1_per_epoch)\n    train_weighted_F1_per_epoch_df.to_csv(\"train_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    #VALIDATION RATING CLASSIFIER ACCURACY\n    val_acc_per_epoch_df = pd.DataFrame(valid_acc_per_epoch)\n    val_acc_per_epoch_df.to_csv(\"val_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #VALIDATION RATING CLASSIFIER WEIGHTED F1\n    val_weighted_F1_per_epoch_df = pd.DataFrame(val_weighted_F1_per_epoch)\n    val_weighted_F1_per_epoch_df.to_csv(\"val_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n\n# END OF ALL FOLDS PROCESSING FOR LOOP\n\n# GET THE ACCURACY RESULTS FILE OF EACH EPOCH\nvf1acc = pd.read_csv(val_fold_1_acc_per_epoch, header = None).to_numpy()\nvf2acc = pd.read_csv(val_fold_2_acc_per_epoch, header = None).to_numpy()\nvf3acc = pd.read_csv(val_fold_3_acc_per_epoch, header = None).to_numpy()\nvf4acc = pd.read_csv(val_fold_4_acc_per_epoch, header = None).to_numpy()\nvf5acc = pd.read_csv(val_fold_5_acc_per_epoch, header = None).to_numpy()\n\n# GET THE WEIGHTED F1 RESULTS FILE OF EACH EPOCH\nvf1f1 = pd.read_csv(val_fold_1_weighted_f1_per_epoch, header = None).to_numpy()\nvf2f1 = pd.read_csv(val_fold_2_weighted_f1_per_epoch, header = None).to_numpy()\nvf3f1 = pd.read_csv(val_fold_3_weighted_f1_per_epoch, header = None).to_numpy()\nvf4f1 = pd.read_csv(val_fold_4_weighted_f1_per_epoch, header = None).to_numpy()\nvf5f1 = pd.read_csv(val_fold_5_weighted_f1_per_epoch, header = None).to_numpy()\n\n\nepoch_acc_avg_for_all_folds = np.zeros((20))\nepoch_f1_avg_for_all_folds = np.zeros((20))\n\n#ITERATE THROUGH EACH EPOCH RESULT OF ALL THE FOLD RESULTS FILE\n#AND STORE THE AVERAGE OF ACROSS ALL FOLDS FOR EACH EPOCH\n# SO THE LIST FINALLY CONSISTS OF AVG RESULTS OF FOLDS FOR INDIVIDUAL EPOCHS\nfor i in range(20):\n    epoch_acc_avg_for_all_folds[i] = (vf1acc[i] + vf2acc[i] + vf3acc[i] + vf4acc[i] +vf5acc[i])/5\n    epoch_f1_avg_for_all_folds[i] = (vf1f1[i] + vf2f1[i] + vf3f1[i] + vf4f1[i] +vf5f1[i])/5\n    \n\n                           \n\n\n\n        ","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# THIS IS SOME MODIFIED TOW ROB IMP. THE ORIGINAL IMP IS IN THE NEXT CELL.\n\n##### THIS IMPLEMENTATION WAS CHANGED TO USE PRETRAINED TEXTCNN MODEL, \n#THE ORIGINAL IMPLEMENTATION IS THE ONE BELOW THIS CELL\n# TRAINING and EVALUATING the GENDER DISCRIMINATOR on the HIDDEN STATE of the TextCNN Model\n# TextCNN Model to use as the baseline.\n\n## Add other evaluation metrics especially F1 score and confusion matrix.(Done)\n#Implement 5 fold cross validation(Done)\n#### First train the text classification model and save the model with the best validation loss.(Done)\n## Make sure you storing the right loss values.(Verify again)\n    \nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torchmetrics import F1Score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn import metrics\n\n\n# START PROCESSING FOR EACH FOLD\n\nfor j in range(1, 6):\n    \n    #LOAD INPUT DATA\n    train_data, valid_data, test_data = data.TabularDataset.splits(path=fold_path,\n                                                                   train=\"train_fold_{}.csv\".format(j),\n                                                                   validation=\"test_fold_{}.csv\".format(j),\n                                                                   test=\"test_fold_{}.csv\".format(j),\n                                                                   fields=[('text', TEXT), ('rating', RATING_LABEL), ('gender', GENDER_LABEL),\n                                                                           ('age', AGE_LABEL), ('location', LOCATION_LABEL)],\n                                                                   format=\"csv\")\n    \n    #GET THE ITERATOR\n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=BATCH_SIZE,\n                                                               device=device,\n                                                               sort_key=lambda x: len(x.text))\n    \n    #BUILD VOCABULARY\n    TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n    RATING_LABEL.build_vocab(train_data)\n    GENDER_LABEL.build_vocab(train_data)\n    AGE_LABEL.build_vocab(train_data)\n    LOCATION_LABEL.build_vocab(train_data)\n    INPUT_DIM = len(TEXT.vocab)\n    \n    \n    ################## CREATE MODELS - TEXT CNN, AND GENDER DISC ##########################\n    \n    # Create an instance of the TEXTCNN Model\n    model = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n    \n    # LOAD PRETRAINED EMBEDDING FOR THE TEXT CNN INPUT\n    model.apply(init_weights)\n    pretrained_embeddings = TEXT.vocab.vectors\n    model.embedding.weight.data.copy_(pretrained_embeddings) \n    \n    # WEIGHTED LOSS FOR TEXT CNN TO HANDLE THE CLASS IMBALANCE ISSUE\n    class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y.numpy())\n    sc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\n    criterion = nn.CrossEntropyLoss(weight = sc).to(device)\n    \n    #LOAD THE TEXT CNN MODEL TRAINED BEFORE\n    #model.load_state_dict(torch.load(textCNNModelPath))\n    \n    #CREATE INSTANCE OF GENDER DISCRIMINATOR\n    discriminator_gender = Discriminator(input_size=(N_FILTERS * len(FILTER_SIZES)),\n                                     hidden_size=HIDDEN_DIM,\n                                     num_classes=2)\n    \n    #LOSS FUNCTION FOR GENDER DISCRIMINATOR\n    criterion_gender = nn.CrossEntropyLoss().to(device)\n    \n    ################################### MODEL CREATION END ###############################\n    \n    \n    # OIPTIMIZERS\n    # FOR TRAINING THE GENDER DISCRIMINATOR\n    optimizer_gender = optim.Adam(discriminator_gender.parameters(), lr=LEARNING_RATE)\n    # FOR TRAINING THE TEXT CNN MODEL - THETA_M(UPTO HIDDEN STATE) \n    # AND THETA_C(FC AFTER HIDDEN STATE)\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # USE THIS OPTIMIZER IF WE ALSO WANT TO TRAIN GENDER DISC ALONG WITH TEXT CNN\n    # NOT DOING THIS AS WE FIRST TRAIN GENDER DISC \n    # AND THEN TRAIN THE THETA_M AND THETA_C TO FOOL GENDER DISC\n    \n    #optimizer = optim.Adam(list(model.parameters()) + list(discriminator_gender.parameters()), lr=LEARNING_RATE)\n\n    model = model.to(device)\n    discriminator_gender = discriminator_gender.to(device)\n\n    \n    ################################## TRAINING STARTS ###################################\n  \n   \n    f1_gender = F1Score(task=\"multiclass\", num_classes=2).to(device)\n\n    #train_loss_per_epoch = []\n    #valid_loss_per_epoch = []\n\n    train_gender_loss_per_epoch = []\n    val_gender_loss_per_epoch = []\n    #train_rating_loss_per_epoch = []\n    #val_rating_loss_per_epoch = []\n\n    train_acc_per_epoch = []\n    valid_acc_per_epoch = []\n    train_gender_acc_per_epoch = []\n    valid_gender_acc_per_epoch = []\n    #train_weighted_F1_per_epoch = []\n    #val_weighted_F1_per_epoch = []\n    train_gender_weighted_F1_per_epoch = []\n    val_gender_weighted_F1_per_epoch = []\n\n    for epoch in range(NUM_EPOCHS):\n\n        #y_tot = torch.empty((0))\n        #pred_tot = torch.empty((0))\n        #y_valid_tot = torch.empty((0))\n        #pred_valid_tot = torch.empty((0))\n        y_gender_tot = torch.empty((0))\n        gender_pred_tot = torch.empty((0))\n        y_gender_valid_tot = torch.empty((0))\n        gender_pred_valid_tot = torch.empty((0))\n\n        model.eval()\n        #discriminator_age.train()\n        discriminator_gender.train()\n        #discriminator_location.train()\n\n        #total_loss = []\n        #total_rating_loss = []\n        total_gender_loss = []\n\n        #train_total_correct = 0\n        #train_age_correct = 0\n        train_gender_correct = 0\n        #train_location_correct = 0\n\n\n        for i, batch in enumerate(train_iter):\n\n            text = batch.text\n            y = batch.rating\n\n            #y_gender = batch.gender\n            #y_age = batch.age\n            #y_location = batch.location\n\n            h = model.hidden_state(text)\n\n\n            \"\"\"\n            Update gender discriminator\n            \"\"\"\n            y_gender = batch.gender\n            y_gender_pred = discriminator_gender(h).squeeze()\n            gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n            gender_loss = criterion_gender(y_gender_pred, y_gender)\n\n            train_gender_correct += (gender_pred == y_gender).sum().item()\n          \n            optimizer_gender.zero_grad()\n            gender_loss.backward()\n            optimizer_gender.step()\n                       \n            total_gender_loss.append(gender_loss.item())\n\n            #y_tot = torch.cat((y_tot, y.cpu()))\n            #pred_tot = torch.cat((pred_tot, rating_pred.cpu()))\n            y_gender_tot = torch.cat((y_gender_tot, y_gender.cpu()))\n            gender_pred_tot = torch.cat((gender_pred_tot, gender_pred.cpu()))\n\n\n            if (i + 1) % 10 == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                      .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, gender_loss.item()))\n\n        #train_loss_per_epoch.append(sum(total_loss)/len(total_loss))\n        #train_rating_loss_per_epoch.append(sum(total_rating_loss)/len(total_rating_loss))\n        train_gender_loss_per_epoch.append(sum(total_gender_loss)/len(total_gender_loss))\n\n        print(\"Training gender avg loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss)/len(total_gender_loss)))\n        print(\"Training total_gender_accuracy = {:.4f}%\".format(100 * train_gender_correct / len(train_data)))\n        print(\"Training total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_tot,y_gender_tot)))\n        \n        #train_acc_per_epoch.append(100 * train_total_correct / len(train_data))\n        train_gender_acc_per_epoch.append(100 * train_gender_correct / len(train_data))\n\n        #conf_mat = confusion_matrix(y_tot.cpu(), pred_tot.cpu())\n\n        #print(conf_mat)\n\n        conf_mat_gender = confusion_matrix(y_gender_tot.cpu(), gender_pred_tot.cpu())\n        print(conf_mat_gender)\n\n        #print(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), digits=3))\n        print(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), digits=3))\n        #train_weighted_F1_per_epoch.append(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        train_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        # Validation\n        \n        discriminator_gender.eval()\n        #total_valid_correct = 0\n        #total_valid_loss = 0.0\n        #valid_age_correct = 0\n        valid_gender_correct = 0\n        #valid_location_correct = 0\n        #total_loss_valid =[]\n        #total_rating_loss_valid =[]\n        total_gender_loss_valid = []\n\n        for i, batch in enumerate(valid_iter):\n            text = batch.text\n            y_valid = batch.rating\n            y_gender_valid = batch.gender\n\n            if text.size()[0] < 5:\n              continue\n            #y_age = batch.age\n            #y_location = batch.location\n\n            #y_pred_valid = model(text).squeeze(1)\n            #h = model.embedding(text).permute(1, 0, 2)\n\n            ## using the correct h\n\n            h = model.hidden_state(text)\n\n            #y_age_pred = discriminator_age(h).squeeze()\n            y_gender_pred_valid = discriminator_gender(h).squeeze()\n            #y_location_pred = discriminator_location(h).squeeze()\n\n            #rating_loss = criterion_rating(y_pred_valid, y_valid)\n            gender_loss = criterion_gender(y_gender_pred_valid, y_gender_valid)\n            #age_loss = criterion_age(y_age_pred, y_age)\n            #location_loss = criterion_location(y_location_pred, y_location)\n\n            #rating_pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n            #total_valid_correct += (rating_pred_valid == y_valid).sum().item()\n            #total_valid_loss += loss.item()\n            #total_loss_valid.append(loss.item())\n            total_gender_loss_valid.append(gender_loss.item())\n\n            #age_pred = torch.argmax(y_age_pred.data, dim=1)\n            #valid_age_correct += (age_pred == y_age).sum().item()\n\n            gender_pred_valid = torch.argmax(y_gender_pred_valid.data, dim=1)\n            valid_gender_correct += (gender_pred_valid == y_gender_valid).sum().item()\n\n            #location_pred = torch.argmax(y_location_pred.data, dim=1)\n            #valid_location_correct += (location_pred == y_location).sum().item()\n\n            #y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n            #pred_valid_tot = torch.cat((pred_valid_tot, rating_pred_valid.cpu()))\n            y_gender_valid_tot = torch.cat((y_gender_valid_tot, y_gender_valid.cpu()))\n            gender_pred_valid_tot = torch.cat((gender_pred_valid_tot, gender_pred_valid.cpu()))\n\n        #valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))  \n        val_gender_loss_per_epoch.append(sum(total_gender_loss_valid)/len(total_gender_loss_valid))\n        #avg_loss = total_valid_loss * BATCH_SIZE / len(valid_data)\n        print(\"Validation gender total_loss = {}\".format(total_gender_loss_valid))\n        print(\"Valid avg loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss_valid)/len(total_gender_loss_valid)))\n        #print(\"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}%\".format(avg_loss, 100 * total_valid_correct / len(valid_data)))\n        #print(\"Validation total_rating_accuracy = {:.4f}%\".format(100 * total_valid_correct / len(valid_data)))\n        #print(\"Validation total_rating_F1  = {:.4f}%\".format(f1_rating(pred_valid_tot, y_valid_tot)))\n        #print(\"Validation total_age_accuracy = {:.4f}%\".format(100 * valid_age_correct / len(valid_data)))\n        #print(\"Validation total_age_F1 = {:.4f}%\".format(f1_age(age_pred,y_age)))\n        print(\"Validation total_gender_accuracy = {:.4f}%\".format(100 * valid_gender_correct / len(valid_data)))\n        print(\"Validation total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_valid_tot,y_gender_valid_tot)))\n        #print(\"Validation total_location_accuracy = {:.4f}%\".format(100 * valid_location_correct / len(valid_data)))\n        #print(\"Validation total_loaction_F1 = {:.4f}%\".format(f1_location(location_pred, y_location)))\n\n\n        #conf_mat = confusion_matrix(y_valid_tot.cpu(), pred_valid_tot.cpu())\n        #valid_acc_per_epoch.append(100 * total_valid_correct / len(valid_data))\n        #val_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        #print(conf_mat)\n\n        conf_mat_gender = confusion_matrix(y_gender_tot.cpu(), gender_pred_tot.cpu())\n        valid_gender_acc_per_epoch.append(100 * valid_gender_correct / len(valid_data))\n        val_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n\n        print(conf_mat_gender)\n\n        #print(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), digits=3))\n        print(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), digits=3))\n        \n    train_acc_per_epoch_df = pd.DataFrame(train_gender_acc_per_epoch)\n    train_acc_per_epoch_df.to_csv(\"train_fold_{}_gender_acc_per_epoch\".format(j), index = False, header = None)\n    train_weighted_F1_per_epoch_df = pd.DataFrame(train_gender_weighted_F1_per_epoch)\n    train_weighted_F1_per_epoch_df.to_csv(\"train_fold_{}_gender_weighted_f1_per_epoch\".format(j), index = False, header = None)\n\n    val_acc_per_epoch_df = pd.DataFrame(valid_gender_acc_per_epoch)\n    val_acc_per_epoch_df.to_csv(\"val_fold_{}_gender_acc_per_epoch\".format(j), index = False, header = None)\n    val_weighted_F1_per_epoch_df = pd.DataFrame(val_gender_weighted_F1_per_epoch)\n    val_weighted_F1_per_epoch_df.to_csv(\"val_fold_{}_gender_weighted_f1_per_epoch\".format(j), index = False, header = None)\n\nvf1acc = pd.read_csv(val_fold_1_gender_acc_per_epoch, header = None).to_numpy()\nvf2acc = pd.read_csv(val_fold_2_gender_acc_per_epoch, header = None).to_numpy()\nvf3acc = pd.read_csv(val_fold_3_gender_acc_per_epoch, header = None).to_numpy()\nvf4acc = pd.read_csv(val_fold_4_gender_acc_per_epoch, header = None).to_numpy()\nvf5acc = pd.read_csv(val_fold_5_gender_acc_per_epoch, header = None).to_numpy()\n\nvf1f1 = pd.read_csv(val_fold_1_gender_weighted_f1_per_epoch, header = None).to_numpy()\nvf2f1 = pd.read_csv(val_fold_2_gender_weighted_f1_per_epoch, header = None).to_numpy()\nvf3f1 = pd.read_csv(val_fold_3_gender_weighted_f1_per_epoch, header = None).to_numpy()\nvf4f1 = pd.read_csv(val_fold_4_gender_weighted_f1_per_epoch, header = None).to_numpy()\nvf5f1 = pd.read_csv(val_fold_5_gender_weighted_f1_per_epoch, header = None).to_numpy()\n\n\nepoch_acc_avg_for_all_folds = np.zeros((20))\nepoch_f1_avg_for_all_folds = np.zeros((20))\nfor i in range(20):\n    epoch_acc_avg_for_all_folds[i] = (vf1acc[i] + vf2acc[i] + vf3acc[i] + vf4acc[i] +vf5acc[i])/5\n    epoch_f1_avg_for_all_folds[i] = (vf1f1[i] + vf2f1[i] + vf3f1[i] + vf4f1[i] +vf5f1[i])/5\n    \n\n                           \n\n\n\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Towards Robust Representation implementation. \n# ORIGINAL IMPLEMENTATION\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torchmetrics import F1Score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn import metrics\n    \n    \n# TRAINING AND VALIDATION FOR A SINGLE FOLD STARTS\nfor j in range(1, 6):\n    \n    #LOAD TRAINING DATA\n    train_data, valid_data, test_data = data.TabularDataset.splits(path=fold_path,\n                                                                   train=\"train_fold_{}.csv\".format(j),\n                                                                   validation=\"test_fold_{}.csv\".format(j),\n                                                                   test=\"test_fold_{}.csv\".format(j),\n                                                                   fields=[('text', TEXT), ('rating', RATING_LABEL), ('gender', GENDER_LABEL),\n                                                                           ('age', AGE_LABEL), ('location', LOCATION_LABEL)],\n                                                                   format=\"csv\")\n    \n    # GET THE ITERATOR\n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=BATCH_SIZE,\n                                                               device=device,\n                                                               sort_key=lambda x: len(x.text))\n    \n    # GET THE VOCABULARY\n    TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n    RATING_LABEL.build_vocab(train_data)\n    GENDER_LABEL.build_vocab(train_data)\n    AGE_LABEL.build_vocab(train_data)\n    LOCATION_LABEL.build_vocab(train_data)\n    \n    INPUT_DIM = len(TEXT.vocab)\n    \n    ################################## MODEL CREATION ####################################\n    \n    # Create an instance of the TEXTCNN Model\n    model = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n    #Add fully connected layer for rating prediction. No need as there is fc layer\n    #at the end of the TextCNN\n    \n    # LOAD PRETRAINED EMBEDDING FOR THE TEXT CNN INPUT\n    model.apply(init_weights)\n    pretrained_embeddings = TEXT.vocab.vectors\n    model.embedding.weight.data.copy_(pretrained_embeddings) \n    \n    # Create a Gender Discriminator instance\n    discriminator_gender = Discriminator(input_size=(N_FILTERS * len(FILTER_SIZES)),\n                                         hidden_size=HIDDEN_DIM,\n                                         num_classes=2)\n    \n    sc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\n    criterion_rating = nn.CrossEntropyLoss(weight = sc).to(device)\n    #criterion_age = nn.CrossEntropyLoss().to(device)\n    criterion_gender = nn.CrossEntropyLoss().to(device)\n    #criterion_location = nn.CrossEntropyLoss().to(device)\n    \n    ################################### MODEL CREATION ENDS ################################\n\n    # Optimizer\n    optimizer_rating = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    #optimizer_age = optim.Adam(discriminator_age.parameters(), lr=LEARNING_RATE)\n    optimizer_gender = optim.Adam(discriminator_gender.parameters(), lr=LEARNING_RATE)\n    #optimizer_location = optim.Adam(discriminator_location.parameters(), lr=LEARNING_RATE)\n\n    optimizer = optim.Adam(list(model.parameters()) + list(discriminator_gender.parameters()), lr=LEARNING_RATE)\n\n    model = model.to(device)\n\n    #discriminator_age = discriminator_age.to(device)\n    discriminator_gender = discriminator_gender.to(device)\n    #discriminator_location = discriminator_location.to(device)\n    \n    \n    \n    total_step = len(train_iter)\n    \n    ################################### TRAINING STARTS ####################################\n\n    f1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\n    #f1_age = F1Score(task=\"multiclass\", num_classes=2).to(device)\n    f1_gender = F1Score(task=\"multiclass\", num_classes=2).to(device)\n    #f1_location = F1Score(task=\"multiclass\", num_classes=5).to(device)\n    \n    # OVERALL LOSS - TRAIN AND VALID\n    train_loss_per_epoch = []\n    valid_loss_per_epoch = []\n    \n    #RATING CLASSIFIER LOSS - TRAIN AND VALID\n    train_rating_loss_per_epoch = []\n    val_rating_loss_per_epoch = []\n    \n    #GENDER DISCRIMINATOR LOSS - TRAIN AND VALID\n    train_gender_loss_per_epoch = []\n    val_gender_loss_per_epoch = []\n    \n    #RATING CLASSIFIER ACCURACY - TRAIN AND VALID\n    train_rating_acc_per_epoch = []\n    valid_rating_acc_per_epoch = []\n    \n    #GENDER DISCRIMINATOR ACCURACY - TRAIN AND VALID\n    train_gender_acc_per_epoch = []\n    valid_gender_acc_per_epoch = []\n    \n    #RATING CLASSIFIER WEIGHTED F1 - TRAIN AND VALID\n    train_rating_weighted_F1_per_epoch = []\n    val_rating_weighted_F1_per_epoch = []\n    \n    #GENDER DISCRIMINATOR WEIGHTED F1 -TRAIN AND VALID\n    train_gender_weighted_F1_per_epoch = []\n    val_gender_weighted_F1_per_epoch = []\n    \n    ####################################### EPOCH TRAINING STARTS ########################\n\n    for epoch in range(NUM_EPOCHS):\n        \n        #GROUND TRUTH RATING FOR ONE EPOCH\n        y_rating_tot = torch.empty((0))\n        y_valid_rating_tot = torch.empty((0))\n       \n        #RATING CLASSIFIER OUTPUT FOR ONE EPOCH\n        pred_rating_tot = torch.empty((0))\n        pred_valid_rating_tot = torch.empty((0))\n        \n        #GROUND TRUTH GENDER FOR ONE EPOCH\n        y_gender_tot = torch.empty((0))\n        y_gender_valid_tot = torch.empty((0))\n        \n        #GENDER DISCRIMINATOR OUTPUT FOR ONE EPOCH\n        gender_pred_tot = torch.empty((0))\n        gender_pred_valid_tot = torch.empty((0))\n        \n        #SET THE MODELS IN TRAIN MODE\n        model.train()\n        #discriminator_age.train()\n        discriminator_gender.train()\n        #discriminator_location.train()\n        \n        \n        #APPEND THE BATCH LOSSES TO THESE LISTS - TOTAL LOSS, RATING CLASSIFIER LOSS \n        #AND GENDER DISCRIMINATOR LOSS\n        total_loss = []\n        total_rating_loss = []\n        total_gender_loss = []\n        \n        #STORE THE NUMBER OF CORRECT PREDICTIONS\n        train_total_rating_correct = 0\n        #train_age_correct = 0\n        train_gender_correct = 0\n        #train_location_correct = 0\n        \n        ########################## BATCH TRAINING STARTS ################################\n        \n        for i, batch in enumerate(train_iter):\n            \n            # LOAD THE DATA - TEXT, RATING, GENDER\n            \n            text = batch.text\n            y = batch.rating\n            y_gender = batch.gender\n            \n            #y_age = batch.age\n            #y_location = batch.location\n            \n            #GET THE HIDDEN STATE OF THE TEXTCNN MODEL\n            #THIS HIDDEN STATE IS THE INPUT TO THE GENDER DISCRIMINATOR\n            #THIS IS TEXT REPRESENTATION WE LEARN\n            \n            \n            \n            ###################### GENDER DISCRIMINATOR TRAINING STARTS #####################\n            \n            h = model.hidden_state(text)\n\n            # GENDER DISCRIMINATOR PREDICTION\n            y_gender_pred = discriminator_gender(h).squeeze()\n            gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n            \n            # LOSS IS CALCULATED ON THE PROBABILITY OUTPUT AND NOT THE DISCRETE OUTPUT\n            gender_loss = LAMBDA * criterion_gender(y_gender_pred, y_gender)\n\n            # UPDATE THE GENDER DISCRIMINATOR PARAMETER \n            # TO MINIMIZE THE GENDER PREDICTION LOSS \n            #optimizer_rating.zero_grad()\n            #optimizer_age.zero_grad()\n            optimizer_gender.zero_grad()\n            gender_loss.backward(retain_graph=True)\n            optimizer_gender.step()\n            #optimizer_location.zero_grad()\n            \n            ##################### GENDER DISCRIMINATOR TRAINING ENDS #########################\n            \n            ########## RATING CLASSIFIER HIDDEN STATE AND OUTPUT TRAINING STARTS #############\n            \n            # TRAIN THETA_M(UPTO HIDDEN STATE) AND THETA_C(LAST FC LAYER) OF TEXTCNN MODEL\n            \n            # GET GENDER DISCRIMINATOR PREDICTION \n            y_gender_pred = discriminator_gender(h).squeeze()\n            gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n            \n            # CALCULATE THE NUMBER OF CORRECT PREDICTION BY GENDER DISCRIMINATOR\n            train_gender_correct += (gender_pred == y_gender).sum().item()\n            gender_loss = criterion_gender(y_gender_pred, y_gender)\n            \n            # UPDATE THE TEXT CNN(RATING CLASSIFIER AND HIDDEN STATE) PARAMETERS \n            optimizer_rating.zero_grad()\n            # Forward pass\n            # y_pred = model(text).squeeze(1).float()\n            y_pred = model(text).squeeze(1)\n            \n            \n            #### QUESTION - IF y_valid is probability based or discrete. \n            # WHY IS y_valid used for loss calculation as well as total_valid_Correct??\n            # GET MODEL PREDICTION ON BATCH VALIDATION DATA\n\n            #discriminator_loss = gender_loss + age_loss + location_loss\n            rating_loss = criterion_rating(y_pred, y)\n            discriminator_loss = gender_loss\n            loss = rating_loss - LAMBDA * discriminator_loss\n            \n            # CALCULATE TOTAL CORRECT RATING CLASSIFIER PREDICTIONS\n            rating_pred = torch.argmax(y_pred.data, dim=1)\n            train_total_rating_correct += (rating_pred == y).sum().item()\n\n            # Backward and optimize\n            loss.backward()\n            optimizer_rating.step()\n            \n            ########## RATING CLASSIFIER HIDDEN STATE AND OUTPUT TRAINING ENDS #############\n            \n            # STORE THE CURRENT BATCH LOSS - ADVERSARIAL LOSS, RATING CLASSIFIER LOSS,\n            # GENDER DISCRIMINATOR LOSS\n            \n            total_loss.append(loss.item())\n            total_rating_loss.append(rating_loss.item())\n            total_gender_loss.append(gender_loss.item())\n            \n            # APPEND THE BATCH OUTPUT TO OVERALL EPOCH DATA\n            \n            #GROUND TRUTH RATING\n            y_rating_tot = torch.cat((y_rating_tot, y.cpu()))\n            \n            #RATING CLASSIFIER OUTPUT\n            pred_rating_tot = torch.cat((pred_rating_tot, rating_pred.cpu()))\n            \n            #GROUND TRUTH GENDER \n            y_gender_tot = torch.cat((y_gender_tot, y_gender.cpu()))\n            \n            #GENDER DISCRIMINATOR OUTPUT\n            gender_pred_tot = torch.cat((gender_pred_tot, gender_pred.cpu()))\n\n            # EPOCH/TOTAL EPOCHS, CURRENT BATCH STEP, TOTAL BATCH STEPS IN EPOCH, ADV LOSS\n            if (i + 1) % 10 == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                      .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n                \n        ###################### BATCH TRAINING ENDS #######################################\n        \n        \n        ########################## STORE THE RESULTS #####################################\n        \n        # OVERALL LOSS- ADVERSARIAL = RATING CLASSIFIER LOSS - LAMBDA * GENDER DISCRIMINATOR LOSS\n        train_loss_per_epoch.append(sum(total_loss)/len(total_loss))\n        \n        # RATING CLASSIFIER LOSS\n        train_rating_loss_per_epoch.append(sum(total_rating_loss)/len(total_rating_loss))\n        \n        # GENDER DISCRIMINATOR LOSS\n        train_gender_loss_per_epoch.append(sum(total_gender_loss)/len(total_gender_loss))\n        \n        # RATING CLASSIFIER - ACCURACY AND WEIGHTED F1 - PER EPOCH\n        train_rating_acc_per_epoch.append(100 * train_total_rating_correct / len(train_data))\n        train_rating_weighted_F1_per_epoch.append(metrics.classification_report(y_rating_tot.cpu(), pred_rating_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        # GENDER DISCRIMINATOR - ACCURACY AND WEIGHTED F1 - PER EPOCH\n        train_gender_acc_per_epoch.append(100 * train_gender_correct / len(train_data))\n        train_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n       \n        # CONFUSION MATRIX \n        conf_mat_rating = confusion_matrix(y_rating_tot.cpu(), pred_rating_tot.cpu())\n        conf_mat_gender = confusion_matrix(y_gender_tot.cpu(), gender_pred_tot.cpu())\n        \n        ########################### END STORE THE RESULTS ##########################################\n        \n        ########################### PRINT THE RESULTS ############################################## \n        \n        #print(\"Training total_loss = {}\".format(total_loss))\n        \n        #PRINT LOSS - OVERALL LOSS(ADVERSARIAL), RATING CLASSIFIER LOSS, GENDER DISCRIMINATOR LOSS\n        print(\"Training total avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss)/len(total_loss)))\n        print(\"Training rating avg loss for the epoch {} = {}\".format(epoch+1, sum(total_rating_loss)/len(total_rating_loss)))\n        print(\"Training gender avg loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss)/len(total_gender_loss)))\n        \n        # RATING CLASSIFIER - ACCURACY AND WEIGHTED F1\n        print(\"Training total_rating_accuracy = {:.4f}%\".format(100 * train_total_rating_correct / len(train_data)))\n        print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_rating_tot, y_rating_tot)))\n        \n        # GENDER DISCRIMINATOR - ACCURACY AND WEIGHTED F1\n        print(\"Training total_gender_accuracy = {:.4f}%\".format(100 * train_gender_correct / len(train_data)))\n        print(\"Training total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_tot,y_gender_tot)))\n        \n        #CONFUSION MATRIX \n        print(conf_mat_rating)\n        print(conf_mat_gender)\n        \n        #METRICS CLASSIFICATION REPORT\n        print(metrics.classification_report(y_rating_tot.cpu(), pred_rating_tot.cpu(), digits=3))\n        print(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), digits=3))\n        \n        ################################### END OF PRINT RESULTS ###############################\n        \n        ################################## EPOCH TRAINING ENDS #################################\n        \n        ################################## EPOCH VALIDATION STARTS #############################\n        \n        # Validation\n        model.eval()\n        \n        # TOTAL CORRECT RATING CLASSIFIER PREDICTION\n        valid_rating_correct = 0\n        valid_gender_correct = 0\n        \n        #VALIDATION LOSS OF MODELS - OVERALL ADV LOSS, RATING CLASSIFIER LOSS \n        # GENDER DISCRIMINATOR LOSS\n        total_valid_loss = 0.0\n        total_rating_valid_loss = 0.0\n        total_gender_valid_loss = 0.0\n        \n        \n        # LIST TO STORE THE OVERALL LOSS(ADV), RATING CLASSIFIER LOSS, GENDER DISC LOSS\n        total_loss_valid =[]\n        total_rating_loss_valid =[]\n        total_gender_loss_valid = []\n        \n        \n        ######################## BATCH VALIDATION STARTS ###################################\n        \n        for i, batch in enumerate(valid_iter):\n            \n            # GET THE DATA - TEXT, RATING and GENDER\n            text = batch.text\n            y_valid = batch.rating\n            y_gender_valid = batch.gender\n            \n            # FOR COMPUTATIONAL REASONS of the model, skipping text lengths less than 5 \n            if text.size()[0] < 5:\n              continue\n            \n            # GET RATING CLASSIFIER OUTPUT\n            y_pred_valid = model(text).squeeze(1)\n            rating_pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n            valid_rating_correct += (rating_pred_valid == y_valid).sum().item()\n            rating_loss_valid = criterion_rating(y_pred_valid, y_valid)\n            #h = model.embedding(text).permute(1, 0, 2)\n\n            ## using the correct h\n            \n            #GET THE HIDDEN STATE OF THE TEXT CNN MODEL\n            h = model.hidden_state(text)\n            \n            #GET GENDER DISCRIMINATOR OUTPUT\n            y_gender_pred_valid = discriminator_gender(h).squeeze()\n            gender_pred_valid = torch.argmax(y_gender_pred_valid.data, dim=1)\n            valid_gender_correct += (gender_pred_valid == y_gender_valid).sum().item()\n            gender_loss_valid = criterion_gender(y_gender_pred_valid, y_gender_valid)\n            \n            #CALCULATE OVERALL ADVERSARIAL LOSS\n            loss_valid = rating_loss_valid - LAMBDA * gender_loss_valid\n            \n            #STORING THE LOSS FOR BATCH DATA - \n            #RATING CLASSIFIER LOSS, GENDER DISC LOSS, ADVERSARIAL LOSS\n            total_valid_rating_loss += rating_loss_valid.item()\n            total_gender_valid_loss += gender_loss_valid.item()\n            total_valid_loss += loss_valid.item()\n            \n            total_rating_loss_valid.append(rating_loss_valid.item())\n            total_gender_loss_valid.append(gender_loss_valid.item())\n            total_loss_valid.append(loss_valid.item())\n            \n            \n            #STORE THE RESULTS - GROUND TRUTH RATING, RATING CLASSIFIER PREDICTION\n            # GROUND TRUTH GENDER,GENDER DISCRIMINATOR PREDICTION\n            \n            #GROUND TRUTH RATING\n            y_valid_rating_tot = torch.cat((y_valid_rating_tot, y_valid.cpu()))\n            \n            #RATING CLASSIFIER PREDICTION\n            pred_valid_rating_tot = torch.cat((pred_valid_rating_tot, rating_pred_valid.cpu()))\n            \n            #GROUND TRUTH GENDER\n            y_gender_valid_tot = torch.cat((y_gender_valid_tot, y_gender_valid.cpu()))\n            \n            #GENDER DISCRIMINATOR PREDICTION\n            gender_pred_valid_tot = torch.cat((gender_pred_valid_tot, gender_pred_valid.cpu()))\n            \n        ####################### END OF BATCH PROCESSING #######################################\n        \n        \n        ####################### STORING THE RESULTS ##########################################\n        \n        #LOSS PER EPOCH\n        valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))  \n        val_rating_loss_per_epoch.append(sum(total_rating_loss_valid)/len(total_rating_loss_valid))\n        val_gender_loss_per_epoch.append(sum(total_gender_loss_valid)/len(total_gender_loss_valid))\n        \n        #ACCURACY PER EPOCH\n        valid_rating_acc_per_epoch.append(100 * valid_rating_correct / len(valid_data))\n        valid_gender_acc_per_epoch.append(100 * valid_gender_correct / len(valid_data))\n        \n        #WEIGHTED F1 PER EPOCH\n        val_rating_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_rating_tot.cpu(), pred_valid_rating_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        val_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        #CONFUSION MATRIX\n        conf_mat_rating = confusion_matrix(y_valid_rating_tot.cpu(), pred_valid_rating_tot.cpu())\n        conf_mat_gender = confusion_matrix(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu())\n        \n        ##################################### END STORING RESULTS ###############################\n        \n        #################################### PRINTING THE RESULTS ################################\n        \n        print(\"Validation total_loss = {}\".format(total_valid_loss))\n        \n        # LOSS PER EPOCH - \n        print(\"Valid avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss_valid)/len(total_loss_valid)))\n        print(\"Valid rating classifier avg loss for the epoch {} = {}\".format(epoch+1, sum(total_rating_loss_valid)/len(total_rating_loss_valid)))\n        print(\"Valid gender discriminator avg loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss_valid)/len(total_gender_loss_valid)))\n        \n        avg_loss = total_valid_loss * BATCH_SIZE / len(valid_data)\n        print(\"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}%\".format(avg_loss, 100 * total_valid_correct / len(valid_data)))\n        \n        # RATING CLASSIFIER - ACCURACY + F1\n        print(\"Validation total_rating_accuracy = {:.4f}%\".format(100 * valid_rating_correct / len(valid_data)))\n        print(\"Validation total_rating_F1  = {:.4f}%\".format(f1_rating(pred_valid_rating_tot, y_valid_rating_tot)))\n        \n        # GENDER DISCRIMINATOR - ACCURACY + F1\n        print(\"Validation total_gender_accuracy = {:.4f}%\".format(100 * valid_gender_correct / len(valid_data)))\n        print(\"Validation total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_valid_tot,y_gender_valid_tot)))\n        \n        # CONFUSION MATRIX\n        print(conf_mat_rating)\n        print(conf_mat_gender)\n        \n        #METRICS CLASSIFICATION REPORT\n        print(metrics.classification_report(y_valid_rating_tot.cpu(), pred_valid_rating_tot.cpu(), digits=3))\n        print(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), digits=3))\n    \n        ################################ END OF PRINT RESULTS #####################################\n        \n        \n    #####################################END OF VALIDATION AND EPOCH ###############################\n    \n    ############################ STORE CURRENT FOLD RESULTS ################################\n    \n    #CONVERT TO DATAFRAME AND \n    #DOWNLOAD AS CSV TRAIN RATING ACCURACY FILE \n    #CONSISTING OF ALL 20 EPOCHS RESULT FOR CURRENT FOLD\n    \n    train_acc_per_epoch_df = pd.DataFrame(train_acc_per_epoch)\n    train_acc_per_epoch_df.to_csv(\"train_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #CONVERT TO DATAFRAME \n    #AND DOWNLOAD AS CSV TRAIN RATING WEIGHTED F1 FILE \n    #CONSISTING OF ALL 20 EPOCHS RESULT FOR CURRENT FOLD\n    train_weighted_F1_per_epoch_df = pd.DataFrame(train_weighted_F1_per_epoch)\n    train_weighted_F1_per_epoch_df.to_csv(\"train_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    #CONVERT TO DATAFRAME \n    #AND DOWNLOAD AS CSV VALIDATION RATING ACCURACY FILE \n    #CONSISTING OF ALL 20 EPOCHS RESULT FOR CURRENT FOLD\n    val_acc_per_epoch_df = pd.DataFrame(valid_acc_per_epoch)\n    val_acc_per_epoch_df.to_csv(\"val_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #CONVERT TO DATAFRAME \n    #AND DOWNLOAD AS CSV VALIDATION RATING WEIGHTED F1 FILE \n    #CONSISTING OF ALL 20 EPOCHS RESULT FOR CURRENT FOLD\n    \n    val_weighted_F1_per_epoch_df = pd.DataFrame(val_weighted_F1_per_epoch)\n    val_weighted_F1_per_epoch_df.to_csv(\"val_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_acc_avg_for_all_folds_df = pd.DataFrame(epoch_acc_avg_for_all_folds)\nepoch_acc_avg_for_all_folds_df.to_csv(\"epoch_acc_avg_for_all_folds_TextCNN.csv\", index = False, header = None)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T21:09:25.576406Z","iopub.execute_input":"2024-01-08T21:09:25.576868Z","iopub.status.idle":"2024-01-08T21:09:25.583763Z","shell.execute_reply.started":"2024-01-08T21:09:25.576821Z","shell.execute_reply":"2024-01-08T21:09:25.582687Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"epoch_f1_avg_for_all_folds_df = pd.DataFrame(epoch_f1_avg_for_all_folds)\nepoch_f1_avg_for_all_folds_df.to_csv(\"epoch_f1_avg_for_all_folds_TextCNN.csv\", index = False, header = None)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T21:09:30.225528Z","iopub.execute_input":"2024-01-08T21:09:30.225924Z","iopub.status.idle":"2024-01-08T21:09:30.232297Z","shell.execute_reply.started":"2024-01-08T21:09:30.225889Z","shell.execute_reply":"2024-01-08T21:09:30.231305Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"epoch_acc_avg_for_all_folds_df","metadata":{"execution":{"iopub.status.busy":"2024-01-08T20:57:46.620257Z","iopub.execute_input":"2024-01-08T20:57:46.621262Z","iopub.status.idle":"2024-01-08T20:57:46.633906Z","shell.execute_reply.started":"2024-01-08T20:57:46.621223Z","shell.execute_reply":"2024-01-08T20:57:46.633116Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"            0\n0   30.780662\n1   22.435428\n2   17.349452\n3   26.764933\n4   38.855353\n5   44.148809\n6   50.563167\n7   55.241344\n8   59.717909\n9   56.185558\n10  55.949661\n11  52.427485\n12  56.180952\n13  59.240507\n14  56.682663\n15  56.446053\n16  54.227785\n17  56.790865\n18  56.525055\n19  56.008549","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30.780662</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22.435428</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17.349452</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26.764933</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>38.855353</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>44.148809</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>50.563167</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>55.241344</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>59.717909</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>56.185558</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>55.949661</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>52.427485</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>56.180952</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>59.240507</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>56.682663</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>56.446053</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>54.227785</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>56.790865</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>56.525055</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>56.008549</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"epoch_acc_avg_for_all_folds","metadata":{"execution":{"iopub.status.busy":"2024-01-08T20:46:10.687316Z","iopub.execute_input":"2024-01-08T20:46:10.687751Z","iopub.status.idle":"2024-01-08T20:46:10.694939Z","shell.execute_reply.started":"2024-01-08T20:46:10.687717Z","shell.execute_reply":"2024-01-08T20:46:10.693988Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([30.78066152, 22.43542798, 17.34945236, 26.76493334, 38.85535285,\n       44.1488091 , 50.56316655, 55.24134439, 59.71790911, 56.18555822,\n       55.94966085, 52.42748524, 56.18095157, 59.24050703, 56.68266348,\n       56.4460534 , 54.22778521, 56.79086463, 56.52505492, 56.00854898])"},"metadata":{}}]},{"cell_type":"code","source":"epoch_f1_avg_for_all_folds","metadata":{"execution":{"iopub.status.busy":"2024-01-08T20:46:21.193200Z","iopub.execute_input":"2024-01-08T20:46:21.194039Z","iopub.status.idle":"2024-01-08T20:46:21.201059Z","shell.execute_reply.started":"2024-01-08T20:46:21.193995Z","shell.execute_reply":"2024-01-08T20:46:21.199853Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([0.27000517, 0.24153865, 0.18504739, 0.3340154 , 0.4476059 ,\n       0.49748894, 0.5395644 , 0.5685083 , 0.59141573, 0.57508584,\n       0.57372489, 0.55208256, 0.57281519, 0.59104232, 0.57731058,\n       0.57557152, 0.56296903, 0.57335823, 0.57719485, 0.57233332])"},"metadata":{}}]},{"cell_type":"code","source":"#there might be problems in loading the text embeddings\n\nINPUT_DIM = len(TEXT.vocab)\n\n# Create an instance\nmodel = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n\n#model.apply(init_weights)\npretrained_embeddings = TEXT.vocab.vectors\nmodel.embedding.weight.data.copy_(pretrained_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIM = len(TEXT.vocab)\nmodel = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\nmodel.embedding.weight","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#why does GLOVE make all the embeddings as 0.\nmodel.embedding.weight","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \n\n#class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y.numpy())\nsc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\ncriterion = nn.CrossEntropyLoss(weight = sc).to(device)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \n\n#class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y.numpy())\n#sc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nmodel = model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Train and Validation ##########\n## Add other evaluation metrics especially F1 score and confusion matrix.\n#Implement 5 fold cross validation\n#### First train the text classification model and save the model with the best validation loss.\n## Make sure you storing the right loss values.\nfrom torchmetrics import F1Score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn import metrics\n\n\nf1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\ntotal_step = len(train_iter)\n\nbest_valid_loss = float('inf')\nbest_epoch = -1\n\n#saved_model = model_name.format(SAMPLING_INDEX+1)\n\ntrain_loss_per_epoch = []\nvalid_loss_per_epoch = []\ntrain_acc_per_epoch = []\nvalid_acc_per_epoch = []\ntrain_weighted_F1_per_epoch = []\nval_weighted_F1_per_epoch = []\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    total_loss = []\n    train_total_correct = 0\n    y_tot = torch.empty((0))\n    pred_tot = torch.empty((0))\n    y_valid_tot = torch.empty((0))\n    pred_valid_tot = torch.empty((0))\n\n    for i, batch in enumerate(train_iter):\n\n        text = batch.text\n        y = batch.rating\n\n        #print(text)\n        #print(text.shape)\n        #print(y_valid)\n        #print(y_valid.shape)\n\n        # Forward pass\n        # y_pred = model(text).squeeze(1).float()\n        y_pred = model(text).squeeze(1)\n        \n        #loss for entire batch size\n        loss = criterion(y_pred, y)\n\n        pred = torch.argmax(y_pred.data, dim=1)\n\n        #it calculates the number of correct predictions at every batch and adds it to the next batch for all the training data\n        train_total_correct += (pred == y).sum().item()\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # len(total_loss) = len(train_data)/batch_size\n        total_loss.append(loss.item())\n\n        if (i + 1) % 10 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                  .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n\n        y_tot = torch.cat((y_tot, y.cpu()))\n        pred_tot = torch.cat((pred_tot, pred.cpu()))\n \n    \n    #every element is the total loss for the batch\n    train_loss_per_epoch.append(sum(total_loss)/len(total_loss))   \n    print(\"Training total_loss = {}\".format(total_loss))\n    print(\"Training avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss)/len(total_loss)))\n\n    #this is correct\n    print(\"Training total_accuracy = {:.4f}%\".format(100 * train_total_correct / len(train_data)))\n    print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_tot.cpu(), y_tot.cpu())))\n    \n    train_acc_per_epoch.append(100 * train_total_correct / len(train_data))\n    conf_mat = confusion_matrix(y_tot.cpu(), pred_tot.cpu())\n\n    print(conf_mat)\n\n    print(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), digits=3))\n\n    train_weighted_F1_per_epoch.append(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n\n    # Validation\n    model.eval()\n    total_valid_correct = 0\n    valid_loss = 0.0\n    total_loss_valid =[]\n    for i, batch in enumerate(valid_iter):\n\n        text = batch.text\n        y_valid = batch.rating\n        \n        \n\n        #print(text)\n        #print(y_valid)\n        if text.size()[0] < 5:\n          continue\n\n        y_pred_valid = model(text).squeeze(1)\n\n        v_loss = criterion(y_pred_valid, y_valid)\n\n        pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n        total_valid_correct += (pred_valid == y_valid).sum().item()\n        valid_loss += v_loss.item()\n        total_loss_valid.append(v_loss.item())\n\n        y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n        pred_valid_tot = torch.cat((pred_valid_tot, pred_valid.cpu()))\n\n    if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            best_epoch = epoch\n    #        torch.save(model.state_dict(), saved_model)\n    \n    #if epoch == 9:\n        #torch.save(model.state_dict(), 'TextCNNmodel_epoch9.pt')\n        \n    \n    \n\n    valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))   \n    print(\"Valid total_loss = {}\".format(total_loss_valid))\n    print(\"Valid avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss_valid)/len(total_loss_valid)))\n    \n    # this is wrong because valid loss sums up the total loss for a batch and thus should be divided by len(valid_data)/batch_size\n    avg_loss = valid_loss * BATCH_SIZE/ len(valid_data)\n    print(\"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}, F1: {:.4f}%\\n\".format(avg_loss, 100 * total_valid_correct / len(valid_data), f1_rating(pred_valid_tot.cpu(), y_valid_tot.cpu())))\n    conf_mat = confusion_matrix(y_valid_tot.cpu(), pred_valid_tot.cpu())\n\n    valid_acc_per_epoch.append(100 * total_valid_correct / len(valid_data))\n    val_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n    print(conf_mat)\n\n    print(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), digits=3))\n    \n    train_acc_per_epoch_df = pd.DataFrame(train_acc_per_epoch)\n    train_acc_per_epoch_df.to_csv(train_fold_3_acc_per_epoch, index = False, header = None)\n    train_weighted_F1_per_epoch_df = pd.DataFrame(train_weighted_F1_per_epoch)\n    train_weighted_F1_per_epoch_df.to_csv(train_fold_3_weighted_f1_per_epoch, index = False, header = None)\n\n    val_acc_per_epoch_df = pd.DataFrame(valid_acc_per_epoch)\n    val_acc_per_epoch_df.to_csv(val_fold_3_acc_per_epoch, index = False, header = None)\n    val_weighted_F1_per_epoch_df = pd.DataFrame(val_weighted_F1_per_epoch)\n    val_weighted_F1_per_epoch_df.to_csv(val_fold_3_weighted_f1_per_epoch, index = False, header = None)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss_per_epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_loss_per_epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_weighted_F1_per_epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain_acc_per_epoch_df = pd.DataFrame(train_acc_per_epoch)\ntrain_acc_per_epoch_df.to_csv(train_fold_3_acc_per_epoch, index = False, header = None)\ntrain_weighted_F1_per_epoch_df = pd.DataFrame(train_weighted_F1_per_epoch)\ntrain_weighted_F1_per_epoch_df.to_csv(train_fold_3_weighted_f1_per_epoch, index = False, header = None)\n\nval_acc_per_epoch_df = pd.DataFrame(valid_acc_per_epoch)\nval_acc_per_epoch_df.to_csv(val_fold_3_acc_per_epoch, index = False, header = None)\nval_weighted_F1_per_epoch_df = pd.DataFrame(val_weighted_F1_per_epoch)\nval_weighted_F1_per_epoch_df.to_csv(val_fold_3_weighted_f1_per_epoch, index = False, header = None)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_fold_1_weighted_f1_per_epoch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, input):\n        # 64,231, 50(batch_size, text length, embedding dim)\n        out = self.fc1(input)\n        #print(\"First, out.size() = \", out.size())\n         #[64,231,50] x [50, 300] = [64,231,300]\n        out = self.relu(out)\n        #print(\"Second, out.size() = \", out.size())\n\n        out = self.fc2(out)\n        # [64,231,300] x [300 x num_classes] = [64,231,num_classes]\n        #print(\"Third, out.size() = \", out.size())\n        out = out.view(input.shape[0], -1)\n        #print(\"Fourth, out.size() = \", out.size())\n\n        out = F.log_softmax(out, dim=1)\n        #print(\"Fifth, out.size() = \", out.size())\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-01-09T05:58:24.869687Z","iopub.execute_input":"2024-01-09T05:58:24.870059Z","iopub.status.idle":"2024-01-09T05:58:24.877224Z","shell.execute_reply.started":"2024-01-09T05:58:24.870027Z","shell.execute_reply":"2024-01-09T05:58:24.876320Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Parameters\nINPUT_DIM = len(TEXT.vocab)\n\n# Create an instance\nmodel = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n#Add fully connected layer for rating prediction. No need as there is fc layer\n#at the end of the TextCNN\n\n\n# Create an Age Discriminator instance\n#discriminator_age = Discriminator(input_size=(N_FILTERS * len(FILTER_SIZES)),\n#                                  hidden_size=HIDDEN_DIM,\n#                                  num_classes=2)\n\n# Create a Gender Discriminator instance\ndiscriminator_gender = Discriminator(input_size=(N_FILTERS * len(FILTER_SIZES)),\n                                     hidden_size=HIDDEN_DIM,\n                                     num_classes=2)\n\n# Create a Location Discriminator instance\n#discriminator_location = Discriminator(input_size=(N_FILTERS * len(FILTER_SIZES)),\n#                                       hidden_size=HIDDEN_DIM,\n#                                       num_classes=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \n\n#class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y.numpy())\nsc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\ncriterion = nn.CrossEntropyLoss(weight = sc).to(device)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nmodel = model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\ncriterion_rating = nn.CrossEntropyLoss(weight = sc).to(device)\n#criterion_age = nn.CrossEntropyLoss().to(device)\ncriterion_gender = nn.CrossEntropyLoss().to(device)\n#criterion_location = nn.CrossEntropyLoss().to(device)\n\n# Optimizer\noptimizer_rating = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n#optimizer_age = optim.Adam(discriminator_age.parameters(), lr=LEARNING_RATE)\noptimizer_gender = optim.Adam(discriminator_gender.parameters(), lr=LEARNING_RATE)\n#optimizer_location = optim.Adam(discriminator_location.parameters(), lr=LEARNING_RATE)\n\n#optimizer = optim.Adam(list(model.parameters()) + list(discriminator_age.parameters()) +\n#                       list(discriminator_gender.parameters()) + list(discriminator_location.parameters()),\n#                       lr=LEARNING_RATE)\n\noptimizer = optim.Adam(list(model.parameters()) + list(discriminator_gender.parameters()), lr=LEARNING_RATE)\n\nmodel = model.to(device)\n\n#discriminator_age = discriminator_age.to(device)\ndiscriminator_gender = discriminator_gender.to(device)\n#discriminator_location = discriminator_location.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Train and Validation ##########\n\n#Training the entire model together. This was my initial training step, and Step 1 mentioned in the evernote notes training details.\n\nfrom torchmetrics import F1Score\ntotal_step = len(train_iter)\n\nf1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\n#f1_age = F1Score(task=\"multiclass\", num_classes=2).to(device)\nf1_gender = F1Score(task=\"multiclass\", num_classes=2).to(device)\n#f1_location = F1Score(task=\"multiclass\", num_classes=5).to(device)\n\nfor epoch in range(NUM_EPOCHS):\n    \n    y_tot = torch.empty((0))\n    pred_tot = torch.empty((0))\n    y_valid_tot = torch.empty((0))\n    pred_valid_tot = torch.empty((0))\n    y_gender_tot = torch.empty((0))\n    gender_pred_tot = torch.empty((0))\n    y_gender_valid_tot = torch.empty((0))\n    gender_pred_valid_tot = torch.empty((0))\n    \n    \n    model.train()\n    #discriminator_age.train()\n    discriminator_gender.train()\n    #discriminator_location.train()\n\n    total_loss = []\n    train_total_correct = 0\n    #train_age_correct = 0\n    train_gender_correct = 0\n    #train_location_correct = 0\n\n\n    for i, batch in enumerate(train_iter):\n\n        text = batch.text\n        y = batch.rating\n        y_gender = batch.gender\n        #y_age = batch.age\n        #y_location = batch.location\n\n        optimizer_rating.zero_grad()\n        #optimizer_age.zero_grad()\n        optimizer_gender.zero_grad()\n        #optimizer_location.zero_grad()\n\n        # Forward pass\n        # y_pred = model(text).squeeze(1).float()\n        y_pred = model(text).squeeze(1)\n\n        #Changes - This is tricky and I need to understand this or if needed modify this.\n        #### h = model.embedding(text).permute(1, 0, 2)\n\n\n        ## using the correct h\n\n        h = model.hidden_state(text)\n\n        #Changes -  This is where I need to change the input dimensions from h to either c or s, in my work.\n        # Also make sure the training of the discriminators is done correctly first.\n        # Otherwise there will be negative loss.\n        #Also determine which parameters are trained at which step.\n\n\n        #y_age_pred = discriminator_age(h).squeeze()\n        y_gender_pred = discriminator_gender(h).squeeze()\n        #y_location_pred = discriminator_location(h).squeeze()\n\n        rating_loss = criterion_rating(y_pred, y)\n        gender_loss = criterion_gender(y_gender_pred, y_gender)\n        #age_loss = criterion_age(y_age_pred, y_age)\n        #location_loss = criterion_location(y_location_pred, y_location)\n\n        #discriminator_loss = gender_loss + age_loss + location_loss\n        discriminator_loss = gender_loss\n        loss = rating_loss - LAMBDA * discriminator_loss\n\n        rating_pred = torch.argmax(y_pred.data, dim=1)\n        train_total_correct += (rating_pred == y).sum().item()\n\n        #age_pred = torch.argmax(y_age_pred.data, dim=1)\n        #train_age_correct += (age_pred == y_age).sum().item()\n\n        gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n        train_gender_correct += (gender_pred == y_gender).sum().item()\n\n        #location_pred = torch.argmax(y_location_pred.data, dim=1)\n        #train_location_correct += (location_pred == y_location).sum().item()\n\n        # Backward and optimize\n        loss.backward()\n        # discriminator_loss.backward()\n\n        optimizer_rating.step()\n        #optimizer_age.step()\n        optimizer_gender.step()\n        #optimizer_location.step()\n\n        total_loss.append(loss.item())\n        \n        y_tot = torch.cat((y_tot, y.cpu()))\n        pred_tot = torch.cat((pred_tot, rating_pred.cpu()))\n        y_gender_tot = torch.cat((y_gender_tot, y_gender.cpu()))\n        gender_pred_tot = torch.cat((gender_pred_tot, gender_pred.cpu()))\n\n\n        if (i + 1) % 10 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                  .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n\n    print(\"Training total_loss = {}\".format(total_loss))\n    print(\"Training total_rating_accuracy = {:.4f}%\".format(100 * train_total_correct / len(train_data)))\n    print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_tot, y_tot)))\n    #print(\"Training total_age_accuracy = {:.4f}%\".format(100 * train_age_correct / len(train_data)))\n    #print(\"Training total_age_F1 = {:.4f}%\".format(f1_age(age_pred,y_age)))\n    print(\"Training total_gender_accuracy = {:.4f}%\".format(100 * train_gender_correct / len(train_data)))\n    print(\"Training total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_tot,y_gender_tot)))\n    #print(\"Training total_location_accuracy = {:.4f}%\".format(100 * train_location_correct / len(train_data)))\n    #print(\"Training total_loaction_F1 = {:.4f}%\".format(f1_location(location_pred, y_location)))\n\n    # Validation\n    model.eval()\n    total_valid_correct = 0\n    total_valid_loss = 0.0\n    valid_age_correct = 0\n    valid_gender_correct = 0\n    valid_location_correct = 0\n\n    for i, batch in enumerate(valid_iter):\n        text = batch.text\n        y_valid = batch.rating\n        y_gender_valid = batch.gender\n        #y_age = batch.age\n        #y_location = batch.location\n        \n        if text.size()[0] < 5:\n          continue\n\n        y_pred_valid = model(text).squeeze(1)\n        #h = model.embedding(text).permute(1, 0, 2)\n\n        ## using the correct h\n\n        h = model.hidden_state(text)\n\n        #y_age_pred = discriminator_age(h).squeeze()\n        y_gender_pred_valid = discriminator_gender(h).squeeze()\n        #y_location_pred = discriminator_location(h).squeeze()\n\n        rating_loss = criterion_rating(y_pred, y)\n        gender_loss = criterion_gender(y_gender_pred, y_gender)\n        \n        loss = rating_loss - LAMBDA * gender_loss\n        #age_loss = criterion_age(y_age_pred, y_age)\n        #location_loss = criterion_location(y_location_pred, y_location)\n        # the correct loss is missing.\n        rating_pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n        total_valid_correct += (rating_pred_valid == y_valid).sum().item()\n        total_valid_loss += loss.item()\n\n        #age_pred = torch.argmax(y_age_pred.data, dim=1)\n        #valid_age_correct += (age_pred == y_age).sum().item()\n\n        gender_pred_valid = torch.argmax(y_gender_pred_valid.data, dim=1)\n        valid_gender_correct += (gender_pred_valid == y_gender_valid).sum().item()\n\n        #location_pred = torch.argmax(y_location_pred.data, dim=1)\n        #valid_location_correct += (location_pred == y_location).sum().item()\n        \n        y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n        pred_valid_tot = torch.cat((pred_valid_tot, rating_pred_valid.cpu()))\n        y_gender_valid_tot = torch.cat((y_gender_valid_tot, y_gender_valid.cpu()))\n        gender_pred_valid_tot = torch.cat((gender_pred_valid_tot, gender_pred_valid.cpu()))\n\n    avg_loss = total_valid_loss * BATCH_SIZE/ len(valid_data)\n    print(\"Validation total_loss = {}\".format(total_valid_loss))\n    print(\n        \"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}%\".format(avg_loss, 100 * total_valid_correct / len(valid_data)))\n    print(\"Validation total_rating_accuracy = {:.4f}%\".format(100 * total_valid_correct / len(valid_data)))\n    print(\"Validation total_rating_F1  = {:.4f}%\".format(f1_rating(pred_valid_tot, y_valid_tot)))\n    #print(\"Validation total_age_accuracy = {:.4f}%\".format(100 * valid_age_correct / len(valid_data)))\n    #print(\"Validation total_age_F1 = {:.4f}%\".format(f1_age(age_pred,y_age)))\n    print(\"Validation total_gender_accuracy = {:.4f}%\".format(100 * valid_gender_correct / len(valid_data)))\n    print(\"Validation total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_valid_tot,y_gender_valid_tot)))\n    #print(\"Validation total_location_accuracy = {:.4f}%\".format(100 * valid_location_correct / len(valid_data)))\n    #print(\"Validation total_loaction_F1 = {:.4f}%\".format(f1_location(location_pred, y_location)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Train and Validation ##########\n## Add other evaluation metrics especially F1 score and confusion matrix.\n#Implement 10 fold cross validation\n#### First train the text classification model and save the model with the best validation loss.\nfrom torchmetrics import F1Score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn import metrics\n\nf1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\ntotal_step = len(train_iter)\n\nbest_valid_loss = float('inf')\nbest_epoch = -1\n\n#saved_model = model_name.format(SAMPLING_INDEX+1)\n\ntrain_loss_per_epoch = []\nvalid_loss_per_epoch = []\ntrain_acc_per_epoch = []\nvalid_acc_per_epoch = []\ntrain_weighted_F1_per_epoch = []\nval_weighted_F1_per_epoch = []\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    total_loss = []\n    train_total_correct = 0\n    y_tot = torch.empty((0))\n    pred_tot = torch.empty((0))\n    y_valid_tot = torch.empty((0))\n    pred_valid_tot = torch.empty((0))\n\n    for i, batch in enumerate(train_iter):\n\n        text = batch.text\n        y = batch.rating\n\n        #print(text)\n        #print(text.shape)\n        #print(y_valid)\n        #print(y_valid.shape)\n\n        # Forward pass\n        # y_pred = model(text).squeeze(1).float()\n        y_pred = model(text).squeeze(1)\n        \n        #loss for entire batch size\n        loss = criterion(y_pred, y)\n\n        pred = torch.argmax(y_pred.data, dim=1)\n\n        #it calculates the number of correct predictions at every batch and adds it to the next batch for all the training data\n        train_total_correct += (pred == y).sum().item()\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # len(total_loss) = len(train_data)/batch_size\n        total_loss.append(loss.item())\n\n        if (i + 1) % 10 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                  .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n\n        y_tot = torch.cat((y_tot, y.cpu()))\n        pred_tot = torch.cat((pred_tot, pred.cpu()))\n \n    \n    #every element is the total loss for the batch\n    train_loss_per_epoch.append(sum(total_loss)/len(total_loss))   \n    print(\"Training total_loss = {}\".format(total_loss))\n    print(\"Training avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss)/len(total_loss)))\n\n    #this is correct\n    print(\"Training total_accuracy = {:.4f}%\".format(100 * train_total_correct / len(train_data)))\n    print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_tot.cpu(), y_tot.cpu())))\n    \n    train_acc_per_epoch.append(100 * train_total_correct / len(train_data))\n    conf_mat = confusion_matrix(y_tot.cpu(), pred_tot.cpu())\n\n    print(conf_mat)\n\n    print(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), digits=3))\n\n    train_weighted_F1_per_epoch.append(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n\n    # Validation\n    model.eval()\n    total_valid_correct = 0\n    valid_loss = 0.0\n    total_loss_valid =[]\n    for i, batch in enumerate(valid_iter):\n\n        text = batch.text\n        y_valid = batch.rating\n        \n        \n\n        #print(text)\n        #print(y_valid)\n        if text.size()[0] < 5:\n          continue\n\n        y_pred_valid = model(text).squeeze(1)\n\n        loss = criterion(y_pred_valid, y_valid)\n\n        pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n        total_valid_correct += (pred_valid == y_valid).sum().item()\n        valid_loss += loss.item()\n        total_loss_valid.append(loss.item())\n\n        y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n        pred_valid_tot = torch.cat((pred_valid_tot, pred_valid.cpu()))\n\n    if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            best_epoch = epoch\n    #        torch.save(model.state_dict(), saved_model)\n    \n    if epoch == 9:\n        torch.save(model.state_dict(), 'TextCNNmodel_epoch9.pt')\n        \n    \n    \n\n    valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))   \n    print(\"Valid total_loss = {}\".format(total_loss_valid))\n    print(\"Valid avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss_valid)/len(total_loss_valid)))\n    \n    # this is wrong because valid loss sums up the total loss for a batch and thus should be divided by len(valid_data)/batch_size\n    avg_loss = valid_loss * BATCH_SIZE/ len(valid_data)\n    print(\"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}, F1: {:.4f}%\\n\".format(avg_loss, 100 * total_valid_correct / len(valid_data), f1_rating(pred_valid_tot.cpu(), y_valid_tot.cpu())))\n    conf_mat = confusion_matrix(y_valid_tot.cpu(), pred_valid_tot.cpu())\n\n    valid_acc_per_epoch.append(100 * total_valid_correct / len(valid_data))\n    val_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n    print(conf_mat)\n\n    print(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), digits=3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Train and Validation ##########\n\n#Training the entire model together. This was my initial training step, and Step 1 mentioned in the evernote notes training details.\n\nfrom torchmetrics import F1Score\ntotal_step = len(train_iter)\n\nf1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\n#f1_age = F1Score(task=\"multiclass\", num_classes=2).to(device)\nf1_gender = F1Score(task=\"multiclass\", num_classes=2).to(device)\n#f1_location = F1Score(task=\"multiclass\", num_classes=5).to(device)\n\n\ntrain_loss_per_epoch = []\nvalid_loss_per_epoch = []\n\ntrain_gender_loss_per_epoch = []\nval_gender_loss_per_epoch = []\ntrain_rating_loss_per_epoch = []\nval_rating_loss_per_epoch = []\n\ntrain_acc_per_epoch = []\nvalid_acc_per_epoch = []\ntrain_gender_acc_per_epoch = []\nvalid_gender_acc_per_epoch = []\ntrain_weighted_F1_per_epoch = []\nval_weighted_F1_per_epoch = []\ntrain_gender_weighted_F1_per_epoch = []\nval_gender_weighted_F1_per_epoch = []\n\nfor epoch in range(NUM_EPOCHS):\n    \n    y_tot = torch.empty((0))\n    pred_tot = torch.empty((0))\n    y_valid_tot = torch.empty((0))\n    pred_valid_tot = torch.empty((0))\n    y_gender_tot = torch.empty((0))\n    gender_pred_tot = torch.empty((0))\n    y_gender_valid_tot = torch.empty((0))\n    gender_pred_valid_tot = torch.empty((0))\n    \n    model.train()\n    #discriminator_age.train()\n    discriminator_gender.train()\n    #discriminator_location.train()\n\n    total_loss = []\n    total_rating_loss = []\n    total_gender_loss = []\n    \n    train_total_correct = 0\n    #train_age_correct = 0\n    train_gender_correct = 0\n    #train_location_correct = 0\n\n\n    for i, batch in enumerate(train_iter):\n\n        text = batch.text\n        y = batch.rating\n        \n        #y_gender = batch.gender\n        #y_age = batch.age\n        #y_location = batch.location\n        \n        h = model.hidden_state(text)\n        \n        \n        \"\"\"\n        Update gender discriminator\n        \"\"\"\n        y_gender = batch.gender\n        y_gender_pred = discriminator_gender(h).squeeze()\n        gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n        gender_loss = LAMBDA * criterion_gender(y_gender_pred, y_gender)\n        \n\n        #optimizer_rating.zero_grad()\n        #optimizer_age.zero_grad()\n        optimizer_gender.zero_grad()\n        gender_loss.backward(retain_graph=True)\n        optimizer_gender.step()\n        #optimizer_location.zero_grad()\n        \n        \n        \n        y_gender_pred = discriminator_gender(h).squeeze()\n        gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n        train_gender_correct += (gender_pred == y_gender).sum().item()\n        gender_loss = criterion_gender(y_gender_pred, y_gender)\n        \n        optimizer_rating.zero_grad()\n        # Forward pass\n        # y_pred = model(text).squeeze(1).float()\n        y_pred = model(text).squeeze(1)\n\n        #Changes - This is tricky and I need to understand this or if needed modify this.\n        #### h = model.embedding(text).permute(1, 0, 2)\n\n\n        ## using the correct h\n\n        \n\n        #Changes -  This is where I need to change the input dimensions from h to either c or s, in my work.\n        # Also make sure the training of the discriminators is done correctly first.\n        # Otherwise there will be negative loss.\n        #Also determine which parameters are trained at which step.\n\n\n        #y_age_pred = discriminator_age(h).squeeze()\n        #y_gender_pred = discriminator_gender(h).squeeze()\n        #y_location_pred = discriminator_location(h).squeeze()\n\n        \n        #gender_loss = criterion_gender(y_gender_pred, y_gender)\n        #age_loss = criterion_age(y_age_pred, y_age)\n        #location_loss = criterion_location(y_location_pred, y_location)\n\n        #discriminator_loss = gender_loss + age_loss + location_loss\n        rating_loss = criterion_rating(y_pred, y)\n        discriminator_loss = gender_loss\n        loss = rating_loss - LAMBDA * discriminator_loss\n\n        rating_pred = torch.argmax(y_pred.data, dim=1)\n        train_total_correct += (rating_pred == y).sum().item()\n\n        #age_pred = torch.argmax(y_age_pred.data, dim=1)\n        #train_age_correct += (age_pred == y_age).sum().item()\n\n        #gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n        \n\n        #location_pred = torch.argmax(y_location_pred.data, dim=1)\n        #train_location_correct += (location_pred == y_location).sum().item()\n\n        # Backward and optimize\n        loss.backward()\n        # discriminator_loss.backward()\n\n        optimizer_rating.step()\n        #optimizer_age.step()\n        #optimizer_gender.step()\n        #optimizer_location.step()\n\n        total_loss.append(loss.item())\n        total_rating_loss.append(rating_loss.item())\n        total_gender_loss.append(gender_loss.item())\n        \n        y_tot = torch.cat((y_tot, y.cpu()))\n        pred_tot = torch.cat((pred_tot, rating_pred.cpu()))\n        y_gender_tot = torch.cat((y_gender_tot, y_gender.cpu()))\n        gender_pred_tot = torch.cat((gender_pred_tot, gender_pred.cpu()))\n\n\n        if (i + 1) % 10 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                  .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n            \n    train_loss_per_epoch.append(sum(total_loss)/len(total_loss))\n    train_rating_loss_per_epoch.append(sum(total_rating_loss)/len(total_rating_loss))\n    train_gender_loss_per_epoch.append(sum(total_gender_loss)/len(total_gender_loss))\n\n    #print(\"Training total_loss = {}\".format(total_loss))\n    print(\"Training total avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss)/len(total_loss)))\n    print(\"Training rating avg loss for the epoch {} = {}\".format(epoch+1, sum(total_rating_loss)/len(total_rating_loss)))\n    print(\"Training gender avg loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss)/len(total_gender_loss)))\n    print(\"Training total_rating_accuracy = {:.4f}%\".format(100 * train_total_correct / len(train_data)))\n    print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_tot, y_tot)))\n    #print(\"Training total_age_accuracy = {:.4f}%\".format(100 * train_age_correct / len(train_data)))\n    #print(\"Training total_age_F1 = {:.4f}%\".format(f1_age(age_pred,y_age)))\n    print(\"Training total_gender_accuracy = {:.4f}%\".format(100 * train_gender_correct / len(train_data)))\n    print(\"Training total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_tot,y_gender_tot)))\n    #print(\"Training total_location_accuracy = {:.4f}%\".format(100 * train_location_correct / len(train_data)))\n    #print(\"Training total_loaction_F1 = {:.4f}%\".format(f1_location(location_pred, y_location)))\n    \n    train_acc_per_epoch.append(100 * train_total_correct / len(train_data))\n    train_gender_acc_per_epoch.append(100 * train_gender_correct / len(train_data))\n    \n    conf_mat = confusion_matrix(y_tot.cpu(), pred_tot.cpu())\n    \n    print(conf_mat)\n    \n    conf_mat_gender = confusion_matrix(y_gender_tot.cpu(), gender_pred_tot.cpu())\n    print(conf_mat_gender)\n\n    print(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), digits=3))\n    print(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), digits=3))\n    train_weighted_F1_per_epoch.append(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n    train_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n    # Validation\n    model.eval()\n    total_valid_correct = 0\n    total_valid_loss = 0.0\n    valid_age_correct = 0\n    valid_gender_correct = 0\n    valid_location_correct = 0\n    total_loss_valid =[]\n    total_rating_loss_valid =[]\n    total_gender_loss_valid = []\n\n    for i, batch in enumerate(valid_iter):\n        text = batch.text\n        y_valid = batch.rating\n        y_gender_valid = batch.gender\n        \n        if text.size()[0] < 5:\n          continue\n        #y_age = batch.age\n        #y_location = batch.location\n\n        y_pred_valid = model(text).squeeze(1)\n        #h = model.embedding(text).permute(1, 0, 2)\n\n        ## using the correct h\n\n        h = model.hidden_state(text)\n\n        #y_age_pred = discriminator_age(h).squeeze()\n        y_gender_pred_valid = discriminator_gender(h).squeeze()\n        #y_location_pred = discriminator_location(h).squeeze()\n\n        rating_loss = criterion_rating(y_pred, y)\n        gender_loss = criterion_gender(y_gender_pred, y_gender)\n        #age_loss = criterion_age(y_age_pred, y_age)\n        #location_loss = criterion_location(y_location_pred, y_location)\n\n        rating_pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n        total_valid_correct += (rating_pred_valid == y_valid).sum().item()\n        total_valid_loss += loss.item()\n        total_loss_valid.append(loss.item())\n\n        #age_pred = torch.argmax(y_age_pred.data, dim=1)\n        #valid_age_correct += (age_pred == y_age).sum().item()\n\n        gender_pred_valid = torch.argmax(y_gender_pred_valid.data, dim=1)\n        valid_gender_correct += (gender_pred_valid == y_gender_valid).sum().item()\n\n        #location_pred = torch.argmax(y_location_pred.data, dim=1)\n        #valid_location_correct += (location_pred == y_location).sum().item()\n        \n        y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n        pred_valid_tot = torch.cat((pred_valid_tot, rating_pred_valid.cpu()))\n        y_gender_valid_tot = torch.cat((y_gender_valid_tot, y_gender_valid.cpu()))\n        gender_pred_valid_tot = torch.cat((gender_pred_valid_tot, gender_pred_valid.cpu()))\n    \n    valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))  \n    avg_loss = total_valid_loss * BATCH_SIZE / len(valid_data)\n    print(\"Validation total_loss = {}\".format(total_valid_loss))\n    rint(\"Valid avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss_valid)/len(total_loss_valid)))\n    print(\"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}%\".format(avg_loss, 100 * total_valid_correct / len(valid_data)))\n    print(\"Validation total_rating_accuracy = {:.4f}%\".format(100 * total_valid_correct / len(valid_data)))\n    print(\"Validation total_rating_F1  = {:.4f}%\".format(f1_rating(pred_valid_tot, y_valid_tot)))\n    #print(\"Validation total_age_accuracy = {:.4f}%\".format(100 * valid_age_correct / len(valid_data)))\n    #print(\"Validation total_age_F1 = {:.4f}%\".format(f1_age(age_pred,y_age)))\n    print(\"Validation total_gender_accuracy = {:.4f}%\".format(100 * valid_gender_correct / len(valid_data)))\n    print(\"Validation total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_valid_tot,y_gender_valid_tot)))\n    #print(\"Validation total_location_accuracy = {:.4f}%\".format(100 * valid_location_correct / len(valid_data)))\n    #print(\"Validation total_loaction_F1 = {:.4f}%\".format(f1_location(location_pred, y_location)))\n    \n   \n    conf_mat = confusion_matrix(y_valid_tot.cpu(), pred_valid_tot.cpu())\n    valid_acc_per_epoch.append(100 * total_valid_correct / len(valid_data))\n    val_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n    print(conf_mat)\n    \n    conf_mat_gender = confusion_matrix(y_gender_tot.cpu(), gender_pred_tot.cpu())\n    valid_gender_acc_per_epoch.append(100 * valid_gender_correct / len(valid_data))\n    val_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n    \n    print(conf_mat_gender)\n\n    print(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), digits=3))\n    print(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), digits=3))","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfold1 = pd.read_csv(val_fold_1_acc_per_epoch, header = None)\nfold2 = pd.read_csv(val_fold_2_acc_per_epoch, header = None)\nfold3 = pd.read_csv(val_fold_3_acc_per_epoch, header = None)\nfold4 = pd.read_csv(val_fold_4_acc_per_epoch, header = None)\nfold5 = pd.read_csv(val_fold_5_acc_per_epoch, header = None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold1_arr = fold1.to_numpy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}