{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7189395,"sourceType":"datasetVersion","datasetId":4156759},{"sourceId":7288363,"sourceType":"datasetVersion","datasetId":4226698},{"sourceId":7367272,"sourceType":"datasetVersion","datasetId":4152755}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Get the relative dataset paths of the input data\n#prefix = '/kaggle/input/robust-and-privacy-preserving-text-representations/Robust_and_Privacy_preserving_Text_Representations-master/Robust_and_Privacy_preserving_Text_Representations-master/'\nprefix = '/kaggle/input/robust-and-privacy-preserving-text-representations/Robust_and_Privacy_preserving_Text_Representations-master-20231212T123340Z-001/Robust_and_Privacy_preserving_Text_Representations-master/Robust_and_Privacy_preserving_Text_Representations-master/'\nprefix2 = '/kaggle/working/'\nraw_dataset_path = prefix + 'dataset/TrustPilot/'\ndataset_path = prefix + 'dataset/WWW2015_processed/'\nfold_path = dataset_path + 'StratifiedFold/'\nprefix3 = '/kaggle/input/resultsrobustprivacy/'\nresult_path = prefix3\ntrain_csv = prefix + 'dataset/WWW2015_processed/train.csv'\ntest_csv = prefix + 'dataset/WWW2015_processed/test.csv'\nvalid_csv = prefix + 'dataset/WWW2015_processed/valid.csv'\ntotal_csv = prefix + 'dataset/WWW2015_processed/total.csv'\ntrain_csv_modified = prefix + 'dataset/WWW2015_processed/train_modified.csv'\ntest_csv_modified = prefix + 'dataset/WWW2015_processed/test_modified.csv'\nvalid_csv_modified = prefix + 'dataset/WWW2015_processed/valid_modified.csv'\ntotal_csv_modified = prefix + 'dataset/WWW2015_processed/total_modified.csv'\n\ntrain_fold_1 = prefix + 'dataset/WWW2015_processed/train_fold_1.csv'\ntest_fold_1 = prefix + 'dataset/WWW2015_processed/test_fold_1.csv'\n\ntrain_fold_2 = prefix + 'dataset/WWW2015_processed/train_fold_2.csv'\ntest_fold_2 = prefix + 'dataset/WWW2015_processed/test_fold_2.csv'\n\ntrain_fold_3 = prefix + 'dataset/WWW2015_processed/train_fold_3.csv'\ntest_fold_3 = prefix + 'dataset/WWW2015_processed/test_fold_3.csv'\n\ntrain_fold_4 = prefix + 'dataset/WWW2015_processed/train_fold_4.csv'\ntest_fold_4 = prefix + 'dataset/WWW2015_processed/test_fold_4.csv'\n\ntrain_fold_5 = prefix + 'dataset/WWW2015_processed/train_fold_5.csv'\ntest_fold_5 = prefix + 'dataset/WWW2015_processed/test_fold_5.csv'\n\ntrain_fold_1_acc_per_epoch = prefix2 + 'train_fold_1_acc_per_epoch.csv'\ntrain_fold_1_weighted_f1_per_epoch = prefix2 + 'train_fold_1_weighted_f1_per_epoch.csv'\nval_fold_1_acc_per_epoch = prefix2 + 'val_fold_1_acc_per_epoch.csv'\nval_fold_1_weighted_f1_per_epoch = prefix2 + 'val_fold_1_weighted_f1_per_epoch.csv'\n\ntrain_fold_2_acc_per_epoch = prefix2 + 'train_fold_2_acc_per_epoch.csv'\ntrain_fold_2_weighted_f1_per_epoch = prefix2 + 'train_fold_2_weighted_f1_per_epoch.csv'\nval_fold_2_acc_per_epoch = prefix2 + 'val_fold_2_acc_per_epoch.csv'\nval_fold_2_weighted_f1_per_epoch = prefix2 + 'val_fold_2_weighted_f1_per_epoch.csv'\n\ntrain_fold_3_acc_per_epoch = prefix2 + 'train_fold_3_acc_per_epoch.csv'\ntrain_fold_3_weighted_f1_per_epoch = prefix2 + 'train_fold_3_weighted_f1_per_epoch.csv'\nval_fold_3_acc_per_epoch = prefix2 + 'val_fold_3_acc_per_epoch.csv'\nval_fold_3_weighted_f1_per_epoch = prefix2 + 'val_fold_3_weighted_f1_per_epoch.csv'\n\ntrain_fold_4_acc_per_epoch = prefix2 + 'train_fold_4_acc_per_epoch.csv'\ntrain_fold_4_weighted_f1_per_epoch = prefix2 + 'train_fold_4_weighted_f1_per_epoch.csv'\nval_fold_4_acc_per_epoch = prefix2 + 'val_fold_4_acc_per_epoch.csv'\nval_fold_4_weighted_f1_per_epoch = prefix2 + 'val_fold_4_weighted_f1_per_epoch.csv'\n\ntrain_fold_5_acc_per_epoch = prefix + 'train_fold_5_acc_per_epoch.csv'\ntrain_fold_5_weighted_f1_per_epoch = prefix + 'train_fold_5_weighted_f1_per_epoch.csv'\nval_fold_5_acc_per_epoch = prefix + 'val_fold_5_acc_per_epoch.csv'\nval_fold_5_weighted_f1_per_epoch = prefix + 'val_fold_5_weighted_f1_per_epoch.csv'\n\ntrain_fold_1_gender_acc_per_epoch = prefix2 + 'train_fold_1_gender_acc_per_epoch.csv'\ntrain_fold_1_gender_weighted_f1_per_epoch = prefix2 + 'train_fold_1_gender_weighted_f1_per_epoch.csv'\nval_fold_1_gender_acc_per_epoch = prefix2 + 'val_fold_1_gender_acc_per_epoch.csv'\nval_fold_1_gender_weighted_f1_per_epoch = prefix2 + 'val_fold_1_gender_weighted_f1_per_epoch.csv'\n\ntrain_fold_2_gender_acc_per_epoch = prefix2 + 'train_fold_2_gender_acc_per_epoch.csv'\ntrain_fold_2_gender_weighted_f1_per_epoch = prefix2 + 'train_fold_2_gender_weighted_f1_per_epoch.csv'\nval_fold_2_gender_acc_per_epoch = prefix2 + 'val_fold_2_gender_acc_per_epoch.csv'\nval_fold_2_gender_weighted_f1_per_epoch = prefix2 + 'val_fold_2_gender_weighted_f1_per_epoch.csv'\n\ntrain_fold_3_gender_acc_per_epoch = prefix2 + 'train_fold_3_gender_acc_per_epoch.csv'\ntrain_fold_3_gender_weighted_f1_per_epoch = prefix2 + 'train_fold_3_gender_weighted_f1_per_epoch.csv'\nval_fold_3_gender_acc_per_epoch = prefix2 + 'val_fold_3_gender_acc_per_epoch.csv'\nval_fold_3_gender_weighted_f1_per_epoch = prefix2 + 'val_fold_3_gender_weighted_f1_per_epoch.csv'\n\ntrain_fold_4_gender_acc_per_epoch = prefix2 + 'train_fold_4_gender_acc_per_epoch.csv'\ntrain_fold_4_gender_weighted_f1_per_epoch = prefix2 + 'train_fold_4_gender_weighted_f1_per_epoch.csv'\nval_fold_4_gender_acc_per_epoch = prefix2 + 'val_fold_4_gender_acc_per_epoch.csv'\nval_fold_4_gender_weighted_f1_per_epoch = prefix2 + 'val_fold_4_gender_weighted_f1_per_epoch.csv'\n\ntrain_fold_5_gender_acc_per_epoch = prefix + 'train_fold_5_gender_acc_per_epoch.csv'\ntrain_fold_5_gender_weighted_f1_per_epoch = prefix + 'train_fold_5_gender_weighted_f1_per_epoch.csv'\nval_fold_5_gender_acc_per_epoch = prefix + 'val_fold_5_gender_acc_per_epoch.csv'\nval_fold_5_gender_weighted_f1_per_epoch = prefix + 'val_fold_5_gender_weighted_f1_per_epoch.csv'\n\nprefix3 = '/kaggle/input/resultsrobustprivacy/'\ntrain_fold_1_acc_per_epoch_results = prefix3 + 'train_fold_1_acc_per_epoch.csv'\ntrain_fold_1_weighted_f1_per_epoch_results = prefix3 + 'train_fold_1_weighted_f1_per_epoch.csv'\nval_fold_1_acc_per_epoch_results = prefix3 + 'val_fold_1_acc_per_epoch.csv'\nval_fold_1_weighted_f1_per_epoch_results = prefix3 + 'val_fold_1_weighted_f1_per_epoch.csv'\n\ntrain_fold_2_acc_per_epoch_results = prefix3 + 'train_fold_2_acc_per_epoch.csv'\ntrain_fold_2_weighted_f1_per_epoch_results = prefix3 + 'train_fold_2_weighted_f1_per_epoch.csv'\nval_fold_2_acc_per_epoch_results = prefix3 + 'val_fold_2_acc_per_epoch.csv'\nval_fold_2_weighted_f1_per_epoch_results = prefix3 + 'val_fold_2_weighted_f1_per_epoch.csv'\n\ntrain_fold_3_acc_per_epoch_results = prefix3 + 'train_fold_3_acc_per_epoch.csv'\ntrain_fold_3_weighted_f1_per_epoch_results = prefix3 + 'train_fold_3_weighted_f1_per_epoch.csv'\nval_fold_3_acc_per_epoch_results = prefix3 + 'val_fold_3_acc_per_epoch.csv'\nval_fold_3_weighted_f1_per_epoch_results = prefix3 + 'val_fold_3_weighted_f1_per_epoch.csv'\n\ntrain_fold_4_acc_per_epoch_results = prefix3 + 'train_fold_4_acc_per_epoch.csv'\ntrain_fold_4_weighted_f1_per_epoch_results = prefix3 + 'train_fold_4_weighted_f1_per_epoch.csv'\nval_fold_4_acc_per_epoch_results = prefix3 + 'val_fold_4_acc_per_epoch.csv'\nval_fold_4_weighted_f1_per_epoch_results = prefix3 + 'val_fold_4_weighted_f1_per_epoch.csv'\n\ntrain_fold_5_acc_per_epoch_results = prefix3 + 'train_fold_5_acc_per_epoch.csv'\ntrain_fold_5_weighted_f1_per_epoch_results = prefix3 + 'train_fold_5_weighted_f1_per_epoch.csv'\nval_fold_5_acc_per_epoch_results = prefix3 + 'val_fold_5_acc_per_epoch.csv'\nval_fold_5_weighted_f1_per_epoch_results = prefix3 + 'val_fold_5_weighted_f1_per_epoch.csv'\n\ntextCNNModelPath = '/kaggle/input/robust-and-privacy-preserving-text-representations/'\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change torchtext version to 0.6.0 and then restart and clear outputs.\n!pip install torchtext==0.6.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchtext\ntorchtext.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchtext\nfrom torchtext.data import get_tokenizer\nfrom torchtext import data, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport spacy\nspacy.load(\"en_core_web_sm\")\n\nimport random\n\nSEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nTEXT = data.Field('spacy')\nRATING_LABEL = data.LabelField()\nGENDER_LABEL = data.LabelField()\nAGE_LABEL = data.LabelField()\nLOCATION_LABEL = data.LabelField()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hyperparameters\n\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nEMBEDDING_DIM = 100\nN_FILTERS = 128\nFILTER_SIZES = [3, 4, 5]\nOUTPUT_DIM = 5\nDROPOUT = 0.2\nNUM_EPOCHS = 20\n\nLAMBDA = 1e-3\nHIDDEN_DIM = 300","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextCNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n                 dropout):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        self.conv_0 = nn.Conv2d(in_channels=1,\n                                out_channels=n_filters,\n                                kernel_size=(filter_sizes[0], embedding_dim))\n\n        self.conv_1 = nn.Conv2d(in_channels=1,\n                                out_channels=n_filters,\n                                kernel_size=(filter_sizes[1], embedding_dim))\n\n        self.conv_2 = nn.Conv2d(in_channels=1,\n                                out_channels=n_filters,\n                                kernel_size=(filter_sizes[2], embedding_dim))\n\n        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text):\n        # text = [sent len, batch size]\n        #print(\"CNN First, text.size() = \", text.size())\n        text = text.permute(1, 0)\n        #print(\"CNN Second, text.size() = \", text.size())\n        # text = [batch size, sent len]\n\n        embedded = self.embedding(text)\n        #print(\"CNN Third, embedded.size() = \", embedded.size())\n        # embedded = [batch size, sent len, emb dim]\n\n        embedded = embedded.unsqueeze(1)\n        #print(\"CNN Fourth, embedded.size() = \", embedded.size())\n\n        # embedded = [batch size, 1, sent len, emb dim]\n\n        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n        #print(\"CNN Fifth, conved_2.size() = \", conved_2.size())\n        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n\n        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n        #print(\"CNN Sixth, pooled_2.size() = \", pooled_2.size())\n\n        # torch.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n        # pooled_n = [batch size, n_filters]\n\n        #****** this cat layer is equivalent to h_drop in the tensorflow implementation\n        # this cat layer is the input to the private attribute discriminators\n\n        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n\n        #print(\"CNN Seventh, cat.size() = \", cat.size())\n        ##### cat = [batch size, n_filters * len(filter_sizes)]******#\n\n\n        #print(\"CNN Eighth, self.fc(cat).size() = \", self.fc(cat).size())\n        return self.fc(cat)\n\n    # I added this from the HanXundong Postag implementation and replaced it with the correct Sentiment hidden state.\n    def hidden_state(self, text):\n\n       # text = [sent len, batch size]\n        #print(\"CNN First, text.size() = \", text.size())\n        text = text.permute(1, 0)\n        #print(\"CNN Second, text.size() = \", text.size())\n        # text = [batch size, sent len]\n\n        embedded = self.embedding(text)\n        #print(\"CNN Third, embedded.size() = \", embedded.size())\n        # embedded = [batch size, sent len, emb dim]\n\n        embedded = embedded.unsqueeze(1)\n        #print(\"CNN Fourth, embedded.size() = \", embedded.size())\n\n        # embedded = [batch size, 1, sent len, emb dim]\n\n        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n        #print(\"CNN Fifth, conved_2.size() = \", conved_2.size())\n        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n\n        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n        #print(\"CNN Sixth, pooled_2.size() = \", pooled_2.size())\n\n        # torch.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\n        # pooled_n = [batch size, n_filters]\n\n        #****** this cat layer is equivalent to h_drop in the tensorflow implementation\n        # this cat layer is the input to the private attribute discriminators\n\n        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n\n        return cat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, input):\n        # 64,231, 50(batch_size, text length, embedding dim)\n        out = self.fc1(input)\n        #print(\"First, out.size() = \", out.size())\n         #[64,231,50] x [50, 300] = [64,231,300]\n        out = self.relu(out)\n        #print(\"Second, out.size() = \", out.size())\n\n        out = self.fc2(out)\n        # [64,231,300] x [300 x num_classes] = [64,231,num_classes]\n        #print(\"Third, out.size() = \", out.size())\n        out = out.view(input.shape[0], -1)\n        #print(\"Fourth, out.size() = \", out.size())\n\n        out = F.log_softmax(out, dim=1)\n        #print(\"Fifth, out.size() = \", out.size())\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training and evaluating the Text CNN model for rating classification baseline result\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torchmetrics import F1Score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn import metrics\nimport torchtext\nfrom torchtext.data import get_tokenizer\nfrom torchtext import data, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport spacy\nspacy.load(\"en_core_web_sm\")\n\nimport random\n\nSEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\n#FOR LOOP TO ITERATE FOR EACH FOLDS\nfor j in range(1, 6):\n    \n    #GET THE DATA\n    train_data, valid_data, test_data = data.TabularDataset.splits(path=fold_path,\n                                                                   train=\"train_fold_{}.csv\".format(j),\n                                                                   validation=\"test_fold_{}.csv\".format(j),\n                                                                   test=\"test_fold_{}.csv\".format(j),\n                                                                   fields=[('text', TEXT), ('rating', RATING_LABEL), ('gender', GENDER_LABEL),\n                                                                           ('age', AGE_LABEL), ('location', LOCATION_LABEL)],\n                                                                   format=\"csv\")\n    \n    #GET THE ITERATOR\n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=BATCH_SIZE,\n                                                               device=device,\n                                                               sort_key=lambda x: len(x.text))\n    \n    #BUILD VOCAB\n    TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n    RATING_LABEL.build_vocab(train_data)\n    GENDER_LABEL.build_vocab(train_data)\n    AGE_LABEL.build_vocab(train_data)\n    LOCATION_LABEL.build_vocab(train_data)\n    \n    INPUT_DIM = len(TEXT.vocab)\n\n    # Create an instance of the TextCNN model\n    model = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n\n    #model.apply(init_weights)\n    # USE PRETRAINED GLOVE EMBEDDINGS \n    pretrained_embeddings = TEXT.vocab.vectors\n    model.embedding.weight.data.copy_(pretrained_embeddings) \n    \n    #DEFINE THE LOSS FUNCTION AND OPTIMIZER\n    #class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y.numpy())\n    sc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\n    criterion = nn.CrossEntropyLoss(weight = sc).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    model = model.to(device)\n    \n    ########## Train and Validation ##########\n    ## Add other evaluation metrics especially F1 score and confusion matrix.\n    #Implement 5 fold cross validation\n    #### First train the text classification model and save the model with the best validation loss.\n    ## Make sure you storing the right loss values.\n   \n    #DEFINE MULTICLASS F1 EVALUATION METRIC\n    f1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\n    total_step = len(train_iter)\n\n    best_valid_loss = float('inf')\n    best_epoch = -1\n\n    #saved_model = model_name.format(SAMPLING_INDEX+1)\n    \n    \n    #LOSS_PER_EPOCH\n    train_loss_per_epoch = []\n    valid_loss_per_epoch = []\n    \n    #ACCURACY PER EPOCH\n    train_acc_per_epoch = []\n    valid_acc_per_epoch = []\n    \n    #WEIGHTED F1 PER EPOCH\n    train_weighted_F1_per_epoch = []\n    val_weighted_F1_per_epoch = []\n    \n    ########################### TRAINING STARTS #########################################\n    \n    for epoch in range(NUM_EPOCHS):\n        \n        model.train()\n        \n        #LIST TO STORE THE LOSS OF ENTIRE EPOCH BY APPENDING LOSS OF EACH BATCH\n        total_loss = []\n        train_total_correct = 0\n        \n        #STORE THE GROUND TRUTH AND PREDICTION OUTPUT OF THE TEXTCNN MODEL FOR THE ENTIRE EPOCH\n        \n        #TRAINING GROUND TRUTH AND PREDICTION\n        y_tot = torch.empty((0))\n        pred_tot = torch.empty((0))\n        \n        #VALID GROUND TRUTH AND PREDICTION\n        y_valid_tot = torch.empty((0))\n        pred_valid_tot = torch.empty((0))\n        \n        ########################## BATCH TRAINING STARTS #############################\n        \n        for i, batch in enumerate(train_iter):\n            \n            #GET INPUT FOR A BATCH\n            text = batch.text\n            y = batch.rating\n\n            # Forward pass\n            # y_pred = model(text).squeeze(1).float()\n            \n            #GET TEXT CNN OUTPUT. GIVES THE PREDICTION IN POBABILITIES\n            \n            y_pred = model(text).squeeze(1)\n\n            #GET THE LOSS. DOUBLE CHECK IF THIS LOSS WORKS WITH\n            #PROBABILITY OUTPUT OR DISCRETE\n            #loss for entire batch size\n            \n            ####### IMP QSSSSSS???????##########################\n            \n            ## WHY is y USED FOR CRITERION LOSS AS WELL AS TRAIN_TOTAL_CORRECT CALCULATION???\n            ## y CAN BE PROBABILITY OR ONE HOT ENCODED, SO FIND OUT WHAT IT IS????\n            ### THIS MIGHT BE THE REASON WHY THE LOSS AND ACCURACY DOESN'T MATCH !!\n            ###########################################\n            \n            loss = criterion(y_pred, y)\n            \n            #CHANGE PROBABILITIY BASED OUTPUT TO DISCRETE OUTPUT\n            pred = torch.argmax(y_pred.data, dim=1)\n\n            #ADD NUMBER OF CORRECT PREDICTIONS IN THE CURRENT BATCH TO \n            #THE TOTAL NUMBER OF CORRECT PREDICTIONS OF THE MODEL\n            \n            train_total_correct += (pred == y).sum().item()\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # len(total_loss) = len(train_data)/batch_size\n            #APPEND LOSS FOR EACH BATCH DATA TO LIST\n            \n            total_loss.append(loss.item())\n\n            #PRINT - EPOCH, TOTAL_EPOCHS, BATCH_STEP IN THE EPOCH, BATCH_LOSS\n            if (i + 1) % 10 == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                      .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n            \n            #APPEND BATCH GROUNT TRUTH TO OVERALL GROUND TRUTH\n            y_tot = torch.cat((y_tot, y.cpu()))\n            \n            #APPEND BATCH TEXT CNN OUTPUT TO OVERALL TEXT CNN OUTPUT\n            pred_tot = torch.cat((pred_tot, pred.cpu()))\n            \n        ########################## BATCH TRAINING ENDS #################################\n        \n        \n        ####################### STORE THE RESULTS ######################################\n        \n        #every element is the loss for the batch. \n        #SUM ALL THE BATCH LOSSES FOR A EPOCH AND \n        #DIVIDE BY THE TOTAL BATCH LOSSES IN EPOCH TO STORE AVG LOSS FOR THE CURRENT EPOCH\n        \n        train_loss_per_epoch.append(sum(total_loss)/len(total_loss))\n        \n        # STORE ACCURACY FOR THE CURRENT EPOCH\n        train_acc_per_epoch.append(100 * train_total_correct / len(train_data))\n        \n        #STORE WEIGHTED FOR FOR THE CURRENT EPOCH\n        train_weighted_F1_per_epoch.append(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        # STORE CONFUSION MATRIX OF CURRENT EPOCH\n        conf_mat = confusion_matrix(y_tot.cpu(), pred_tot.cpu())\n        \n        ################# PRINT RESULTS FOR CURRENT EPOCH #############################\n        \n        #PRINT ALL BATCH LOSSES AND AVG FOR THE CURRENT EPOCH\n        print(\"Training total_loss = {}\".format(total_loss))\n        print(\"Training avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss)/len(total_loss)))\n\n        #PRINT ACCURACY FOR EPOCH AND F1 FOR THE CURRENT EPOCH\n        print(\"Training total_accuracy = {:.4f}%\".format(100 * train_total_correct / len(train_data)))\n        print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_tot.cpu(), y_tot.cpu())))\n\n        #PRINT CONFUSION MATRIX\n        print(conf_mat)\n        \n        #PRINT METRICS CLASSIFICATION REPORT\n        print(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), digits=3))\n\n        ####################### TRAINING ENDS ##########################################\n        \n        \n        ####################### VALIDATION STARTS ######################################\n        \n        model.eval()\n        \n        #STORE TOTAL VALIDATION \n        total_valid_correct = 0\n        \n        # CALCULATE VALIDATION LOSS\n        valid_loss = 0.0\n        total_loss_valid =[]\n        \n        ############################ BATCH VALIDATION STARTS ###############################\n        \n        for i, batch in enumerate(valid_iter):\n\n            \n            # GET THE INPUT TEXT AND RATING\n            text = batch.text\n            y_valid = batch.rating\n\n\n\n            #print(text)\n            #print(y_valid)\n            \n            # IF TEXT SIZE IS LESS THAN 5, THEN SKIP THAT DATA POINT FOR COMPUTATIONAL PURPOSES\n            if text.size()[0] < 5:\n              continue\n            \n            \n            #### QUESTION - IF y_valid is probability based or discrete. \n            # WHY IS y_valid used for loss calculation as well as total_valid_Correct??\n            # GET MODEL PREDICTION ON BATCH VALIDATION DATA\n            y_pred_valid = model(text).squeeze(1)\n            pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n            \n            #CALCULATE VALIDATION LOSS\n            v_loss = criterion(y_pred_valid, y_valid)\n            \n            \n\n            # CALCULATE NUMBER OF CORRECT PREDICTIONS\n            total_valid_correct += (pred_valid == y_valid).sum().item()\n            \n            #ADD LOSS FOR EACH BATCH TO VALID LOSS AND LIST\n            valid_loss += v_loss.item()\n            total_loss_valid.append(v_loss.item())\n            \n            #ADD BATCH VALIDATION GROUND TRUTH TO ENTIRE DATA\n            y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n            pred_valid_tot = torch.cat((pred_valid_tot, pred_valid.cpu()))\n\n        if valid_loss < best_valid_loss:\n                best_valid_loss = valid_loss\n                best_epoch = epoch\n        #        torch.save(model.state_dict(), saved_model)\n\n        if epoch == 9:\n            torch.save(model.state_dict(), 'TextCNNmodel_epoch9_fold_{}.pt'.format(j))\n\n\n        ########################### END OF BATCH VALIDATION ##############################\n        \n        \n        ########################### STORE THE RESULTS #####################################\n\n        valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))\n        valid_acc_per_epoch.append(100 * total_valid_correct / len(valid_data))\n        val_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        avg_loss = valid_loss * BATCH_SIZE/ len(valid_data)\n        conf_mat = confusion_matrix(y_valid_tot.cpu(), pred_valid_tot.cpu())\n        \n        ########################### END STORE RESULTS #####################################\n        \n        ########################### PRINT RESULTS #########################################\n        \n        print(\"Valid total_loss = {}\".format(total_loss_valid))\n        print(\"Valid avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss_valid)/len(total_loss_valid)))\n\n        # this is wrong because valid loss sums up the total loss for a batch and thus should be divided by len(valid_data)/batch_size\n        \n        print(\"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}, F1: {:.4f}%\\n\".format(avg_loss, 100 * total_valid_correct / len(valid_data), f1_rating(pred_valid_tot.cpu(), y_valid_tot.cpu())))\n        \n        print(conf_mat)\n\n        print(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), digits=3))\n        \n        ############################ END PRINT RESULTS #####################################\n    \n    ############################ END VALIDATION AND EPOCH TRAINING #########################\n    \n    ############################ STORE CURRENT FOLD RESULTS ################################\n    \n    #CONVERT TO DATAFRAME AND \n    #DOWNLOAD AS CSV - TRAIN RATING ACCURACY FILE, TRAIN RATING F1,\n    #VAL RATING ACCURACY AND VAL RATING F1\n    #CONSISTING OF ALL 20 EPOCHS RESULT FOR CURRENT FOLD\n    \n    #RATING CLASSIFIER ACCURACY\n    train_acc_per_epoch_df = pd.DataFrame(train_acc_per_epoch)\n    train_acc_per_epoch_df.to_csv(\"train_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n        \n    #RATING CLASSIFIER WEIGHTED F1\n    train_weighted_F1_per_epoch_df = pd.DataFrame(train_weighted_F1_per_epoch)\n    train_weighted_F1_per_epoch_df.to_csv(\"train_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    #VALIDATION RATING CLASSIFIER ACCURACY\n    val_acc_per_epoch_df = pd.DataFrame(valid_acc_per_epoch)\n    val_acc_per_epoch_df.to_csv(\"val_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #VALIDATION RATING CLASSIFIER WEIGHTED F1\n    val_weighted_F1_per_epoch_df = pd.DataFrame(val_weighted_F1_per_epoch)\n    val_weighted_F1_per_epoch_df.to_csv(\"val_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n\n# END OF ALL FOLDS PROCESSING FOR LOOP\n\n# GET THE ACCURACY RESULTS FILE OF EACH EPOCH\nvf1acc = pd.read_csv(val_fold_1_acc_per_epoch, header = None).to_numpy()\nvf2acc = pd.read_csv(val_fold_2_acc_per_epoch, header = None).to_numpy()\nvf3acc = pd.read_csv(val_fold_3_acc_per_epoch, header = None).to_numpy()\nvf4acc = pd.read_csv(val_fold_4_acc_per_epoch, header = None).to_numpy()\nvf5acc = pd.read_csv(val_fold_5_acc_per_epoch, header = None).to_numpy()\n\n# GET THE WEIGHTED F1 RESULTS FILE OF EACH EPOCH\nvf1f1 = pd.read_csv(val_fold_1_weighted_f1_per_epoch, header = None).to_numpy()\nvf2f1 = pd.read_csv(val_fold_2_weighted_f1_per_epoch, header = None).to_numpy()\nvf3f1 = pd.read_csv(val_fold_3_weighted_f1_per_epoch, header = None).to_numpy()\nvf4f1 = pd.read_csv(val_fold_4_weighted_f1_per_epoch, header = None).to_numpy()\nvf5f1 = pd.read_csv(val_fold_5_weighted_f1_per_epoch, header = None).to_numpy()\n\n\nepoch_acc_avg_for_all_folds = np.zeros((20))\nepoch_f1_avg_for_all_folds = np.zeros((20))\n\n#ITERATE THROUGH EACH EPOCH RESULT OF ALL THE FOLD RESULTS FILE\n#AND STORE THE AVERAGE OF ACROSS ALL FOLDS FOR EACH EPOCH\n# SO THE LIST FINALLY CONSISTS OF AVG RESULTS OF FOLDS FOR INDIVIDUAL EPOCHS\nfor i in range(20):\n    epoch_acc_avg_for_all_folds[i] = (vf1acc[i] + vf2acc[i] + vf3acc[i] + vf4acc[i] +vf5acc[i])/5\n    epoch_f1_avg_for_all_folds[i] = (vf1f1[i] + vf2f1[i] + vf3f1[i] + vf4f1[i] +vf5f1[i])/5\n    \nepoch_acc_avg_for_all_folds_df = pd.DataFrame(epoch_acc_avg_for_all_folds)\nepoch_acc_avg_for_all_folds_df.to_csv(\"epoch_acc_avg_for_all_folds_TextCNN.csv\", index = False, header = None)                      \n\nepoch_f1_avg_for_all_folds_df = pd.DataFrame(epoch_f1_avg_for_all_folds)\nepoch_f1_avg_for_all_folds_df.to_csv(\"epoch_f1_avg_for_all_folds_TextCNN.csv\", index = False, header = None)\n\n\n        ","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(prefix2 +\"TextCNNResults\", 'zip', prefix2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'TextCNNResults.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# THIS IS MODIFIED TO USE PRETRAINED TEXTCNN AND TRAIN THE GENDER DISCRIMINATOR ON THE HIDDEN STATE TO GET BASELINE GENDER PREDICTIONS\n\n#Note - Have to use a different pretrained TextCNN for each fold data,\n#As the TextCNN trained on a given fold data only works for that fold,\n#Error - And size mismatch for embedding.weight: copying a param with shape torch.Size([45664, 100]) from checkpoint, \n#the shape in current model is torch.Size([45503, 100]).\n\n## Add other evaluation metrics especially F1 score and confusion matrix.(Done)\n#Implement 5 fold cross validation(Done)\n#### First train the text classification model and save the model with the best validation loss.(Done)\n    \nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torchmetrics import F1Score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn import metrics\n\n\n# TRAINING AND VALIDATION FOR A SINGLE FOLD STARTS\nfor j in range(1, 6):\n    \n    #LOAD INPUT DATA\n    train_data, valid_data, test_data = data.TabularDataset.splits(path=fold_path,\n                                                                   train=\"train_fold_{}.csv\".format(j),\n                                                                   validation=\"test_fold_{}.csv\".format(j),\n                                                                   test=\"test_fold_{}.csv\".format(j),\n                                                                   fields=[('text', TEXT), ('rating', RATING_LABEL), ('gender', GENDER_LABEL),\n                                                                           ('age', AGE_LABEL), ('location', LOCATION_LABEL)],\n                                                                   format=\"csv\")\n    \n    #GET THE ITERATOR\n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=BATCH_SIZE,\n                                                               device=device,\n                                                               sort_key=lambda x: len(x.text))\n    \n    #BUILD VOCABULARY\n    TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n    RATING_LABEL.build_vocab(train_data)\n    GENDER_LABEL.build_vocab(train_data)\n    AGE_LABEL.build_vocab(train_data)\n    LOCATION_LABEL.build_vocab(train_data)\n    INPUT_DIM = len(TEXT.vocab)\n    \n    \n    ################## CREATE MODELS - TEXT CNN, AND GENDER DISC ##########################\n    \n    # Create an instance of the TEXTCNN Model\n    model = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n    \n    # LOAD PRETRAINED EMBEDDING FOR THE TEXT CNN INPUT\n    #model.apply(init_weights)\n    pretrained_embeddings = TEXT.vocab.vectors\n    model.embedding.weight.data.copy_(pretrained_embeddings) \n    \n    # WEIGHTED LOSS FOR TEXT CNN TO HANDLE THE CLASS IMBALANCE ISSUE\n    #class_weights=class_weight.compute_class_weight('balanced',np.unique(y),y.numpy())\n    #sc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\n    #criterion = nn.CrossEntropyLoss(weight = sc).to(device)\n    \n    #LOAD THE PRETRAINED TEXT CNN MODEL \n    model.load_state_dict(torch.load(textCNNModelPath+'TextCNNmodel_epoch9_fold_{}.pt'.format(j)))\n    \n    #CREATE INSTANCE OF GENDER DISCRIMINATOR\n    discriminator_gender = Discriminator(input_size=(N_FILTERS * len(FILTER_SIZES)),\n                                     hidden_size=HIDDEN_DIM,\n                                     num_classes=2)\n    \n    #LOSS FUNCTION FOR GENDER DISCRIMINATOR\n    criterion_gender = nn.CrossEntropyLoss().to(device)\n    \n    ################################### MODEL CREATION END ###############################\n    \n    \n    # OPTIMIZERS\n    # FOR TRAINING THE GENDER DISCRIMINATOR\n    optimizer_gender = optim.Adam(discriminator_gender.parameters(), lr=LEARNING_RATE)\n    # FOR TRAINING THE TEXT CNN MODEL - THETA_M(UPTO HIDDEN STATE) \n    # AND THETA_C(FC AFTER HIDDEN STATE)\n    #optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # USE THIS OPTIMIZER IF WE ALSO WANT TO TRAIN GENDER DISC ALONG WITH TEXT CNN\n    # NOT DOING THIS AS WE FIRST TRAIN GENDER DISC \n    # AND THEN TRAIN THE THETA_M AND THETA_C TO FOOL GENDER DISC\n    \n    #optimizer = optim.Adam(list(model.parameters()) + list(discriminator_gender.parameters()), lr=LEARNING_RATE)\n\n    model = model.to(device)\n    discriminator_gender = discriminator_gender.to(device)\n    \n    ################################## TRAINING STARTS ###################################\n  \n    f1_gender = F1Score(task=\"multiclass\", num_classes=2).to(device)\n\n    #train_loss_per_epoch = []\n    #valid_loss_per_epoch = []\n    \n    #GENDER DISCRIMINATOR LOSS - TRAIN AND VALID\n    train_gender_loss_per_epoch = []\n    val_gender_loss_per_epoch = []\n    \n    #train_rating_loss_per_epoch = []\n    #val_rating_loss_per_epoch = []\n\n    #train_acc_per_epoch = []\n    #valid_acc_per_epoch = []\n    \n    #GENDER DISCRIMINATOR ACCURACY - TRAIN AND VALID \n    train_gender_acc_per_epoch = []\n    valid_gender_acc_per_epoch = []\n    \n    #train_weighted_F1_per_epoch = []\n    #val_weighted_F1_per_epoch = []\n    \n    #GENDER DISCRIMINATOR WEIGHTED F1 - TRAIN AND VALID\n    train_gender_weighted_F1_per_epoch = []\n    val_gender_weighted_F1_per_epoch = []\n    \n    ####################################### EPOCH TRAINING STARTS ########################\n\n    for epoch in range(NUM_EPOCHS):\n\n        #y_tot = torch.empty((0))\n        #pred_tot = torch.empty((0))\n        #y_valid_tot = torch.empty((0))\n        #pred_valid_tot = torch.empty((0))\n        \n        #GROUND TRUTH GENDER FOR ONE EPOCH\n        y_gender_tot = torch.empty((0))\n        gender_pred_tot = torch.empty((0))\n        \n        #GENDER DISCRIMINATOR OUTPUT FOR ONE EPOCH\n        y_gender_valid_tot = torch.empty((0))\n        gender_pred_valid_tot = torch.empty((0))\n        \n        #SET TEXTCNN MODEL TO EVAL AS WE ARE USING THE PRETRAINED MODEL\n        model.eval()\n        \n        #discriminator_age.train()\n        \n        # SET DISCRIMINATOR GENDER TO TRAIN \n        discriminator_gender.train()\n        #discriminator_location.train()\n        \n        #APPEND THE BATCH LOSSES TO THESE LISTS - GENDER DISCRIMINATOR LOSS\n        \n        #total_loss = []\n        #total_rating_loss = []\n        total_gender_loss = []\n        \n        #STORE THE NUMBER OF CORRECT PREDICTIONS \n        #train_total_correct = 0\n        #train_age_correct = 0\n        train_gender_correct = 0\n        #train_location_correct = 0\n\n        ################################## BATCH TRAINING STARTS ################################################\n        \n        for i, batch in enumerate(train_iter):\n            \n            #LOAD THE DATA - TEXT, RATING, GENDER\n            text = batch.text\n            y = batch.rating\n            y_gender = batch.gender\n            \n            #y_gender = batch.gender\n            #y_age = batch.age\n            #y_location = batch.location\n            \n            #Get the hidden state of the pretrained Text CNN Model \n            h = model.hidden_state(text)\n\n            \"\"\"\n            Update gender discriminator\n            \"\"\"\n            \n            ###################### GENDER DISCRIMINATOR TRAINING STARTS #####################\n            \n            y_gender_pred = discriminator_gender(h).squeeze()\n            gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n            gender_loss = criterion_gender(y_gender_pred, y_gender)\n\n            train_gender_correct += (gender_pred == y_gender).sum().item()\n          \n            optimizer_gender.zero_grad()\n            gender_loss.backward()\n            optimizer_gender.step()\n            \n            ###################### GENDER DISCRIMINATOR TRAINING ENDS #####################\n                       \n            # STORE THE CURRENT BATCH LOSS - GENDER DISCRIMINATOR LOSS\n            total_gender_loss.append(gender_loss.item())\n            \n            # APPEND THE CURRENT BATCH RESULTS TO EPOCH DATA \n            \n            #y_tot = torch.cat((y_tot, y.cpu()))\n            #pred_tot = torch.cat((pred_tot, rating_pred.cpu()))\n            \n            #GROUND TRUTH GENDER\n            y_gender_tot = torch.cat((y_gender_tot, y_gender.cpu()))\n            \n            #GENDER DISCRIMINATOR PREDICTION\n            gender_pred_tot = torch.cat((gender_pred_tot, gender_pred.cpu()))\n\n\n            if (i + 1) % 10 == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                      .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, gender_loss.item()))\n                \n                \n        ########################################### BATCH TRAINING ENDS ########################################################\n        \n        ########################################## STORE THE RESULTS ###########################################################\n        \n        # GENDER DISCRIMINATOR LOSS\n        train_gender_loss_per_epoch.append(sum(total_gender_loss)/len(total_gender_loss))\n        \n        #GENDER DISCRIMINATOR ACCURACY\n        #train_acc_per_epoch.append(100 * train_total_correct / len(train_data))\n        train_gender_acc_per_epoch.append(100 * train_gender_correct / len(train_data))\n        \n        \n        #GENDER DISCRIMINATOR CONFUSION MATRIX \n        #conf_mat = confusion_matrix(y_tot.cpu(), pred_tot.cpu())\n        conf_mat_gender = confusion_matrix(y_gender_tot.cpu(), gender_pred_tot.cpu())\n        \n        #GENDER DISCRIMINATOR WEIGHTED F1 \n        #train_weighted_F1_per_epoch.append(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        train_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        ######################################### PRINT THE RESULTS #########################################################\n        \n        #GENDER DISCRIMINATOR LOSS\n        print(\"Training gender avg loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss)/len(total_gender_loss)))\n        \n        #GENDER DISCRIMINATOR ACCURACY\n        print(\"Training total_gender_accuracy = {:.4f}%\".format(100 * train_gender_correct / len(train_data)))\n        \n        #GENDER DISCRIMINATOR WEIGHTED F1\n        print(\"Training total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_tot,y_gender_tot)))\n        \n        #GENDER DISCRIMINATOR CONFUSION MATRIX AND METRICS CLASSIFICATION\n        #print(conf_mat)\n        print(conf_mat_gender)\n        #print(metrics.classification_report(y_tot.cpu(), pred_tot.cpu(), digits=3))\n        print(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), digits=3))\n        \n        ################################### END OF PRINT RESULTS ###############################\n        \n        ################################## EPOCH TRAINING ENDS #################################\n        \n        ################################## EPOCH VALIDATION STARTS #############################\n        \n        # Validation\n        \n        discriminator_gender.eval()\n        #total_valid_correct = 0\n        #total_valid_loss = 0.0\n        #valid_age_correct = 0\n        valid_gender_correct = 0\n        #valid_location_correct = 0\n        #total_loss_valid =[]\n        #total_rating_loss_valid =[]\n        total_gender_loss_valid = []\n        \n        ######################## BATCH VALIDATION STARTS ###################################\n\n        for i, batch in enumerate(valid_iter):\n            \n            #LOAD THE DATA - TEXT, RATING, GENDER\n            text = batch.text\n            y_valid = batch.rating\n            y_gender_valid = batch.gender\n\n            if text.size()[0] < 5:\n              continue\n            #y_age = batch.age\n            #y_location = batch.location\n\n            #y_pred_valid = model(text).squeeze(1)\n            #h = model.embedding(text).permute(1, 0, 2)\n\n            ## using the correct h\n            \n            # GET THE HIDDEN STATE\n            h = model.hidden_state(text)\n            \n            #GET GENDER DISCRIMINATOR PREDICTION \n            \n            #y_age_pred = discriminator_age(h).squeeze()\n            y_gender_pred_valid = discriminator_gender(h).squeeze()\n            #y_location_pred = discriminator_location(h).squeeze()\n            \n            #rating_loss = criterion_rating(y_pred_valid, y_valid)\n            gender_loss = criterion_gender(y_gender_pred_valid, y_gender_valid)\n            #age_loss = criterion_age(y_age_pred, y_age)\n            #location_loss = criterion_location(y_location_pred, y_location)\n\n            #rating_pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n            #total_valid_correct += (rating_pred_valid == y_valid).sum().item()\n            #total_valid_loss += loss.item()\n            #total_loss_valid.append(loss.item())\n            total_gender_loss_valid.append(gender_loss.item())\n\n            #age_pred = torch.argmax(y_age_pred.data, dim=1)\n            #valid_age_correct += (age_pred == y_age).sum().item()\n            \n            \n            gender_pred_valid = torch.argmax(y_gender_pred_valid.data, dim=1)\n            valid_gender_correct += (gender_pred_valid == y_gender_valid).sum().item()\n\n            #location_pred = torch.argmax(y_location_pred.data, dim=1)\n            #valid_location_correct += (location_pred == y_location).sum().item()\n            \n            # APPEND THE BATCH VALIDATION DATA TO EPOCH DATA\n            \n            #y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n            #pred_valid_tot = torch.cat((pred_valid_tot, rating_pred_valid.cpu()))\n            \n            #GENDER GROUND TRUTH\n            y_gender_valid_tot = torch.cat((y_gender_valid_tot, y_gender_valid.cpu()))\n            \n            #GENDER DISCRIMINATOR PREDICTION \n            gender_pred_valid_tot = torch.cat((gender_pred_valid_tot, gender_pred_valid.cpu()))\n            \n        ####################### END OF BATCH PROCESSING #######################################\n        \n        ####################### STORING THE RESULTS ############################################\n\n        #valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))\n        \n        #GENDER DISCRIMINATOR LOSS\n        val_gender_loss_per_epoch.append(sum(total_gender_loss_valid)/len(total_gender_loss_valid))\n        \n        #avg_loss = total_valid_loss * BATCH_SIZE / len(valid_data)\n        #conf_mat = confusion_matrix(y_valid_tot.cpu(), pred_valid_tot.cpu())\n        #valid_acc_per_epoch.append(100 * total_valid_correct / len(valid_data))\n        #val_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        #GENDER DISC CONF MATRIX\n        conf_mat_gender = confusion_matrix(y_gender_tot.cpu(), gender_pred_tot.cpu())\n        \n        #GENDER DISC ACCURACY \n        valid_gender_acc_per_epoch.append(100 * valid_gender_correct / len(valid_data))\n        \n        #GENDER DISC WEIGHTED F1\n        val_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        ######################## PRINTING THE RESULTS ############################################\n        \n        #GENDER DISC LOSS\n        print(\"Validation gender total_loss = {}\".format(total_gender_loss_valid))\n        print(\"Valid avg gender disc loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss_valid)/len(total_gender_loss_valid)))\n        \n        #print(\"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}%\".format(avg_loss, 100 * total_valid_correct / len(valid_data)))\n        #print(\"Validation total_rating_accuracy = {:.4f}%\".format(100 * total_valid_correct / len(valid_data)))\n        #print(\"Validation total_rating_F1  = {:.4f}%\".format(f1_rating(pred_valid_tot, y_valid_tot)))\n        #print(\"Validation total_age_accuracy = {:.4f}%\".format(100 * valid_age_correct / len(valid_data)))\n        #print(\"Validation total_age_F1 = {:.4f}%\".format(f1_age(age_pred,y_age)))\n        \n        #GENDER DISC ACCURACY AND WEIGHTED F1\n        print(\"Validation total_gender_accuracy = {:.4f}%\".format(100 * valid_gender_correct / len(valid_data)))\n        print(\"Validation total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_valid_tot,y_gender_valid_tot)))\n        #print(\"Validation total_location_accuracy = {:.4f}%\".format(100 * valid_location_correct / len(valid_data)))\n        #print(\"Validation total_loaction_F1 = {:.4f}%\".format(f1_location(location_pred, y_location)))\n        \n        #GENDER DISCRIMINATOR CONF MATRIX\n        #print(conf_mat)\n        print(conf_mat_gender)\n        \n        #GENDER DISC METRICS CLASSIFICATION\n        #print(metrics.classification_report(y_valid_tot.cpu(), pred_valid_tot.cpu(), digits=3))\n        print(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), digits=3))\n        \n        ############################ END OF PRINT RESULTS ##################################################\n        \n    ##################################### END OF VALIDATION AND EPOCH ###############################\n    \n    ############################ STORE CURRENT FOLD RESULTS ################################\n    \n    #TRAIN GENDER DISCRIMINATOR ACCURACY\n    train_gender_acc_per_epoch_df = pd.DataFrame(train_gender_acc_per_epoch)\n    train_gender_acc_per_epoch_df.to_csv(\"train_fold_{}_gender_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #TRAIN GENDER DISCRIMINATOR WEIGHTED F1\n    train_gender_weighted_F1_per_epoch_df = pd.DataFrame(train_gender_weighted_F1_per_epoch)\n    train_gender_weighted_F1_per_epoch_df.to_csv(\"train_fold_{}_gender_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    #VAL GENDER DISCRIMINATOR ACCURACY\n    val_gender_acc_per_epoch_df = pd.DataFrame(valid_gender_acc_per_epoch)\n    val_gender_acc_per_epoch_df.to_csv(\"val_fold_{}_gender_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #VAL GENDER DISCRIMINATOR WEIGHTED F1\n    val_gender_weighted_F1_per_epoch_df = pd.DataFrame(val_gender_weighted_F1_per_epoch)\n    val_gender_weighted_F1_per_epoch_df.to_csv(\"val_fold_{}_gender_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    ############################# END OF ALL FOLDS PROCESSING ####################################\n    \n    \n#################### CALCULATE THE AVERAGE RESULTS ACROSS FOLDS ###########################################\n\n#GET ACC FOLD RESULTS OF GENDER DISCRIMINATOR\nvf1_gd_acc = pd.read_csv(val_fold_1_gender_acc_per_epoch, header = None).to_numpy()\nvf2_gd_acc = pd.read_csv(val_fold_2_gender_acc_per_epoch, header = None).to_numpy()\nvf3_gd_acc = pd.read_csv(val_fold_3_gender_acc_per_epoch, header = None).to_numpy()\nvf4_gd_acc = pd.read_csv(val_fold_4_gender_acc_per_epoch, header = None).to_numpy()\nvf5_gd_acc = pd.read_csv(val_fold_5_gender_acc_per_epoch, header = None).to_numpy()\n\n#GET F1 FOLD RESULTS OF GENDER DISCRIMINATOR\nvf1_gd_f1 = pd.read_csv(val_fold_1_gender_weighted_f1_per_epoch, header = None).to_numpy()\nvf2_gd_f1 = pd.read_csv(val_fold_2_gender_weighted_f1_per_epoch, header = None).to_numpy()\nvf3_gd_f1 = pd.read_csv(val_fold_3_gender_weighted_f1_per_epoch, header = None).to_numpy()\nvf4_gd_f1 = pd.read_csv(val_fold_4_gender_weighted_f1_per_epoch, header = None).to_numpy()\nvf5_gd_f1 = pd.read_csv(val_fold_5_gender_weighted_f1_per_epoch, header = None).to_numpy()\n\n\nepoch_gender_disc_acc_avg_for_all_folds = np.zeros((20))\nepoch_gender_disc_f1_avg_for_all_folds = np.zeros((20))\n\n#CALCULATE AVG FOLDS RESULTS FOR EACH EPOCH\nfor i in range(20):\n    \n    #GENDER DISCRIMINATOR\n    epoch_gender_disc_acc_avg_for_all_folds[i] = (vf1_gd_acc[i] + vf2_gd_acc[i] + vf3_gd_acc[i] + vf4_gd_acc[i] +vf5_gd_acc[i])/5\n    epoch_gender_disc_f1_avg_for_all_folds[i] = (vf1_gd_f1[i] + vf2_gd_f1[i] + vf3_gd_f1[i] + vf4_gd_f1[i] +vf5_gd_f1[i])/5\n    \n######################### SAVE AVG FOLDS RESULTS FOR EACH EPOCH ############################\n\n#GENDER DISCRIMINATOR ACCURACY\nepoch_gender_disc_acc_avg_for_all_folds_df = pd.DataFrame(epoch_gender_disc_acc_avg_for_all_folds)\nepoch_gender_disc_acc_avg_for_all_folds_df.to_csv(\"epoch_gender_disc_acc_avg_for_all_folds_GenderDiscBaseline.csv\", index = False, header = None)                      \n\n#GENDER DISCRIMINATOR F1\nepoch_gender_disc_f1_avg_for_all_folds_df = pd.DataFrame(epoch_gender_disc_f1_avg_for_all_folds)\nepoch_gender_disc_f1_avg_for_all_folds_df.to_csv(\"epoch_gender_disc_f1_avg_for_all_folds_GenderDiscBaseline.csv\", index = False, header = None)\n                           \n\n\n\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Towards Robust Representation adversarial implementation. \n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torchmetrics import F1Score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn import metrics\n    \n    \n# TRAINING AND VALIDATION FOR A SINGLE FOLD STARTS\nfor j in range(1, 6):\n    \n    #LOAD TRAINING DATA\n    train_data, valid_data, test_data = data.TabularDataset.splits(path=fold_path,\n                                                                   train=\"train_fold_{}.csv\".format(j),\n                                                                   validation=\"test_fold_{}.csv\".format(j),\n                                                                   test=\"test_fold_{}.csv\".format(j),\n                                                                   fields=[('text', TEXT), ('rating', RATING_LABEL), ('gender', GENDER_LABEL),\n                                                                           ('age', AGE_LABEL), ('location', LOCATION_LABEL)],\n                                                                   format=\"csv\")\n    \n    # GET THE ITERATOR\n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=BATCH_SIZE,\n                                                               device=device,\n                                                               sort_key=lambda x: len(x.text))\n    \n    # GET THE VOCABULARY\n    TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n    RATING_LABEL.build_vocab(train_data)\n    GENDER_LABEL.build_vocab(train_data)\n    AGE_LABEL.build_vocab(train_data)\n    LOCATION_LABEL.build_vocab(train_data)\n    \n    INPUT_DIM = len(TEXT.vocab)\n    \n    ################################## MODEL CREATION ####################################\n    \n    # Create an instance of the TEXTCNN Model\n    model = TextCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n    #Add fully connected layer for rating prediction. No need as there is fc layer\n    #at the end of the TextCNN\n    \n    # LOAD PRETRAINED EMBEDDING FOR THE TEXT CNN INPUT\n    #model.apply(init_weights)\n    pretrained_embeddings = TEXT.vocab.vectors\n    model.embedding.weight.data.copy_(pretrained_embeddings) \n    \n    # Create a Gender Discriminator instance\n    discriminator_gender = Discriminator(input_size=(N_FILTERS * len(FILTER_SIZES)),\n                                         hidden_size=HIDDEN_DIM,\n                                         num_classes=2)\n    \n    sc=torch.tensor([1, 8, 8, 24, 24], dtype=torch.float)\n    criterion_rating = nn.CrossEntropyLoss(weight = sc).to(device)\n    #criterion_age = nn.CrossEntropyLoss().to(device)\n    criterion_gender = nn.CrossEntropyLoss().to(device)\n    #criterion_location = nn.CrossEntropyLoss().to(device)\n    \n    ################################### MODEL CREATION ENDS ################################\n\n    # Optimizer\n    optimizer_rating = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    #optimizer_age = optim.Adam(discriminator_age.parameters(), lr=LEARNING_RATE)\n    optimizer_gender = optim.Adam(discriminator_gender.parameters(), lr=LEARNING_RATE)\n    #optimizer_location = optim.Adam(discriminator_location.parameters(), lr=LEARNING_RATE)\n\n    optimizer = optim.Adam(list(model.parameters()) + list(discriminator_gender.parameters()), lr=LEARNING_RATE)\n\n    model = model.to(device)\n\n    #discriminator_age = discriminator_age.to(device)\n    discriminator_gender = discriminator_gender.to(device)\n    #discriminator_location = discriminator_location.to(device)\n    \n    \n    \n    total_step = len(train_iter)\n    \n    ################################### TRAINING STARTS ####################################\n\n    f1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\n    #f1_age = F1Score(task=\"multiclass\", num_classes=2).to(device)\n    f1_gender = F1Score(task=\"multiclass\", num_classes=2).to(device)\n    #f1_location = F1Score(task=\"multiclass\", num_classes=5).to(device)\n    \n    # OVERALL LOSS - TRAIN AND VALID\n    train_loss_per_epoch = []\n    valid_loss_per_epoch = []\n    \n    #RATING CLASSIFIER LOSS - TRAIN AND VALID\n    train_rating_loss_per_epoch = []\n    val_rating_loss_per_epoch = []\n    \n    #GENDER DISCRIMINATOR LOSS - TRAIN AND VALID\n    train_gender_loss_per_epoch = []\n    val_gender_loss_per_epoch = []\n    \n    #RATING CLASSIFIER ACCURACY - TRAIN AND VALID\n    train_rating_acc_per_epoch = []\n    valid_rating_acc_per_epoch = []\n    \n    #GENDER DISCRIMINATOR ACCURACY - TRAIN AND VALID\n    train_gender_acc_per_epoch = []\n    valid_gender_acc_per_epoch = []\n    \n    #RATING CLASSIFIER WEIGHTED F1 - TRAIN AND VALID\n    train_rating_weighted_F1_per_epoch = []\n    val_rating_weighted_F1_per_epoch = []\n    \n    #GENDER DISCRIMINATOR WEIGHTED F1 -TRAIN AND VALID\n    train_gender_weighted_F1_per_epoch = []\n    val_gender_weighted_F1_per_epoch = []\n    \n    ####################################### EPOCH TRAINING STARTS ########################\n\n    for epoch in range(NUM_EPOCHS):\n        \n        #GROUND TRUTH RATING FOR ONE EPOCH\n        y_rating_tot = torch.empty((0))\n        y_valid_rating_tot = torch.empty((0))\n       \n        #RATING CLASSIFIER OUTPUT FOR ONE EPOCH\n        pred_rating_tot = torch.empty((0))\n        pred_valid_rating_tot = torch.empty((0))\n        \n        #GROUND TRUTH GENDER FOR ONE EPOCH\n        y_gender_tot = torch.empty((0))\n        y_gender_valid_tot = torch.empty((0))\n        \n        #GENDER DISCRIMINATOR OUTPUT FOR ONE EPOCH\n        gender_pred_tot = torch.empty((0))\n        gender_pred_valid_tot = torch.empty((0))\n        \n        #SET THE MODELS IN TRAIN MODE\n        model.train()\n        #discriminator_age.train()\n        discriminator_gender.train()\n        #discriminator_location.train()\n        \n        \n        #APPEND THE BATCH LOSSES TO THESE LISTS - TOTAL LOSS, RATING CLASSIFIER LOSS \n        #AND GENDER DISCRIMINATOR LOSS\n        total_loss = []\n        total_rating_loss = []\n        total_gender_loss = []\n        \n        #STORE THE NUMBER OF CORRECT PREDICTIONS\n        train_total_rating_correct = 0\n        #train_age_correct = 0\n        train_gender_correct = 0\n        #train_location_correct = 0\n        \n        ########################## BATCH TRAINING STARTS ################################\n        \n        for i, batch in enumerate(train_iter):\n            \n            # LOAD THE DATA - TEXT, RATING, GENDER\n            \n            text = batch.text\n            y = batch.rating\n            y_gender = batch.gender\n            \n            #y_age = batch.age\n            #y_location = batch.location\n            \n            #GET THE HIDDEN STATE OF THE TEXTCNN MODEL\n            #THIS HIDDEN STATE IS THE INPUT TO THE GENDER DISCRIMINATOR\n            #THIS IS TEXT REPRESENTATION WE LEARN\n            \n            \n            \n            ###################### GENDER DISCRIMINATOR TRAINING STARTS #####################\n            \n            h = model.hidden_state(text)\n\n            # GENDER DISCRIMINATOR PREDICTION\n            y_gender_pred = discriminator_gender(h).squeeze()\n            gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n            \n            # LOSS IS CALCULATED ON THE PROBABILITY OUTPUT AND NOT THE DISCRETE OUTPUT\n            gender_loss = LAMBDA * criterion_gender(y_gender_pred, y_gender)\n\n            # UPDATE THE GENDER DISCRIMINATOR PARAMETER \n            # TO MINIMIZE THE GENDER PREDICTION LOSS \n            #optimizer_rating.zero_grad()\n            #optimizer_age.zero_grad()\n            optimizer_gender.zero_grad()\n            gender_loss.backward(retain_graph=True)\n            optimizer_gender.step()\n            #optimizer_location.zero_grad()\n            \n            ##################### GENDER DISCRIMINATOR TRAINING ENDS #########################\n            \n            ########## RATING CLASSIFIER HIDDEN STATE AND OUTPUT TRAINING STARTS #############\n            \n            # TRAIN THETA_M(UPTO HIDDEN STATE) AND THETA_C(LAST FC LAYER) OF TEXTCNN MODEL\n            \n            # GET GENDER DISCRIMINATOR PREDICTION \n            y_gender_pred = discriminator_gender(h).squeeze()\n            gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n            \n            # CALCULATE THE NUMBER OF CORRECT PREDICTION BY GENDER DISCRIMINATOR\n            train_gender_correct += (gender_pred == y_gender).sum().item()\n            gender_loss = criterion_gender(y_gender_pred, y_gender)\n            \n            # UPDATE THE TEXT CNN(RATING CLASSIFIER AND HIDDEN STATE) PARAMETERS \n            optimizer_rating.zero_grad()\n            # Forward pass\n            # y_pred = model(text).squeeze(1).float()\n            y_pred = model(text).squeeze(1)\n            \n            \n            #### QUESTION - IF y_valid is probability based or discrete. \n            # WHY IS y_valid used for loss calculation as well as total_valid_Correct??\n            # GET MODEL PREDICTION ON BATCH VALIDATION DATA\n\n            #discriminator_loss = gender_loss + age_loss + location_loss\n            rating_loss = criterion_rating(y_pred, y)\n            discriminator_loss = gender_loss\n            loss = rating_loss - LAMBDA * discriminator_loss\n            \n            # CALCULATE TOTAL CORRECT RATING CLASSIFIER PREDICTIONS\n            rating_pred = torch.argmax(y_pred.data, dim=1)\n            train_total_rating_correct += (rating_pred == y).sum().item()\n\n            # Backward and optimize\n            loss.backward()\n            optimizer_rating.step()\n            \n            ########## RATING CLASSIFIER HIDDEN STATE AND OUTPUT TRAINING ENDS #############\n            \n            # STORE THE CURRENT BATCH LOSS - ADVERSARIAL LOSS, RATING CLASSIFIER LOSS,\n            # GENDER DISCRIMINATOR LOSS\n            \n            total_loss.append(loss.item())\n            total_rating_loss.append(rating_loss.item())\n            total_gender_loss.append(gender_loss.item())\n            \n            # APPEND THE BATCH OUTPUT TO OVERALL EPOCH DATA\n            \n            #GROUND TRUTH RATING\n            y_rating_tot = torch.cat((y_rating_tot, y.cpu()))\n            \n            #RATING CLASSIFIER OUTPUT\n            pred_rating_tot = torch.cat((pred_rating_tot, rating_pred.cpu()))\n            \n            #GROUND TRUTH GENDER \n            y_gender_tot = torch.cat((y_gender_tot, y_gender.cpu()))\n            \n            #GENDER DISCRIMINATOR OUTPUT\n            gender_pred_tot = torch.cat((gender_pred_tot, gender_pred.cpu()))\n\n            # EPOCH/TOTAL EPOCHS, CURRENT BATCH STEP, TOTAL BATCH STEPS IN EPOCH, ADV LOSS\n            if (i + 1) % 10 == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                      .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n                \n        ###################### BATCH TRAINING ENDS #######################################\n        \n        \n        ########################## STORE THE RESULTS #####################################\n        \n        # OVERALL LOSS- ADVERSARIAL = RATING CLASSIFIER LOSS - LAMBDA * GENDER DISCRIMINATOR LOSS\n        train_loss_per_epoch.append(sum(total_loss)/len(total_loss))\n        \n        # RATING CLASSIFIER LOSS\n        train_rating_loss_per_epoch.append(sum(total_rating_loss)/len(total_rating_loss))\n        \n        # GENDER DISCRIMINATOR LOSS\n        train_gender_loss_per_epoch.append(sum(total_gender_loss)/len(total_gender_loss))\n        \n        # RATING CLASSIFIER - ACCURACY AND WEIGHTED F1 - PER EPOCH\n        train_rating_acc_per_epoch.append(100 * train_total_rating_correct / len(train_data))\n        train_rating_weighted_F1_per_epoch.append(metrics.classification_report(y_rating_tot.cpu(), pred_rating_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        # GENDER DISCRIMINATOR - ACCURACY AND WEIGHTED F1 - PER EPOCH\n        train_gender_acc_per_epoch.append(100 * train_gender_correct / len(train_data))\n        train_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n       \n        # CONFUSION MATRIX \n        conf_mat_rating = confusion_matrix(y_rating_tot.cpu(), pred_rating_tot.cpu())\n        conf_mat_gender = confusion_matrix(y_gender_tot.cpu(), gender_pred_tot.cpu())\n        \n        ########################### END STORE THE RESULTS ##########################################\n        \n        ########################### PRINT THE RESULTS ############################################## \n        \n        #print(\"Training total_loss = {}\".format(total_loss))\n        \n        #PRINT LOSS - OVERALL LOSS(ADVERSARIAL), RATING CLASSIFIER LOSS, GENDER DISCRIMINATOR LOSS\n        print(\"Training total avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss)/len(total_loss)))\n        print(\"Training rating avg loss for the epoch {} = {}\".format(epoch+1, sum(total_rating_loss)/len(total_rating_loss)))\n        print(\"Training gender avg loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss)/len(total_gender_loss)))\n        \n        # RATING CLASSIFIER - ACCURACY AND WEIGHTED F1\n        print(\"Training total_rating_accuracy = {:.4f}%\".format(100 * train_total_rating_correct / len(train_data)))\n        print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_rating_tot, y_rating_tot)))\n        \n        # GENDER DISCRIMINATOR - ACCURACY AND WEIGHTED F1\n        print(\"Training total_gender_accuracy = {:.4f}%\".format(100 * train_gender_correct / len(train_data)))\n        print(\"Training total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_tot,y_gender_tot)))\n        \n        #CONFUSION MATRIX \n        #print(conf_mat_rating)\n        #print(conf_mat_gender)\n        \n        #METRICS CLASSIFICATION REPORT\n        #print(metrics.classification_report(y_rating_tot.cpu(), pred_rating_tot.cpu(), digits=3))\n        #print(metrics.classification_report(y_gender_tot.cpu(), gender_pred_tot.cpu(), digits=3))\n        \n        ################################### END OF PRINT RESULTS ###############################\n        \n        ################################## EPOCH TRAINING ENDS #################################\n        \n        ################################## EPOCH VALIDATION STARTS #############################\n        \n        # Validation\n        model.eval()\n        \n        # TOTAL CORRECT RATING CLASSIFIER PREDICTION\n        valid_rating_correct = 0\n        valid_gender_correct = 0\n        \n        #VALIDATION LOSS OF MODELS - OVERALL ADV LOSS, RATING CLASSIFIER LOSS \n        # GENDER DISCRIMINATOR LOSS\n        total_valid_loss = 0.0\n        total_rating_valid_loss = 0.0\n        total_gender_valid_loss = 0.0\n        \n        \n        # LIST TO STORE THE OVERALL LOSS(ADV), RATING CLASSIFIER LOSS, GENDER DISC LOSS\n        total_loss_valid =[]\n        total_rating_loss_valid =[]\n        total_gender_loss_valid = []\n        \n        \n        ######################## BATCH VALIDATION STARTS ###################################\n        \n        for i, batch in enumerate(valid_iter):\n            \n            # GET THE DATA - TEXT, RATING and GENDER\n            text = batch.text\n            y_valid = batch.rating\n            y_gender_valid = batch.gender\n            \n            # FOR COMPUTATIONAL REASONS of the model, skipping text lengths less than 5 \n            if text.size()[0] < 5:\n              continue\n            \n            # GET RATING CLASSIFIER OUTPUT\n            y_pred_valid = model(text).squeeze(1)\n            rating_pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n            valid_rating_correct += (rating_pred_valid == y_valid).sum().item()\n            rating_loss_valid = criterion_rating(y_pred_valid, y_valid)\n            #h = model.embedding(text).permute(1, 0, 2)\n\n            ## using the correct h\n            \n            #GET THE HIDDEN STATE OF THE TEXT CNN MODEL\n            h = model.hidden_state(text)\n            \n            #GET GENDER DISCRIMINATOR OUTPUT\n            y_gender_pred_valid = discriminator_gender(h).squeeze()\n            gender_pred_valid = torch.argmax(y_gender_pred_valid.data, dim=1)\n            valid_gender_correct += (gender_pred_valid == y_gender_valid).sum().item()\n            gender_loss_valid = criterion_gender(y_gender_pred_valid, y_gender_valid)\n            \n            #CALCULATE OVERALL ADVERSARIAL LOSS\n            loss_valid = rating_loss_valid - LAMBDA * gender_loss_valid\n            \n            #STORING THE LOSS FOR BATCH DATA - \n            #RATING CLASSIFIER LOSS, GENDER DISC LOSS, ADVERSARIAL LOSS\n            total_rating_valid_loss += rating_loss_valid.item()\n            total_gender_valid_loss += gender_loss_valid.item()\n            total_valid_loss += loss_valid.item()\n            \n            total_rating_loss_valid.append(rating_loss_valid.item())\n            total_gender_loss_valid.append(gender_loss_valid.item())\n            total_loss_valid.append(loss_valid.item())\n            \n            \n            #STORE THE RESULTS - GROUND TRUTH RATING, RATING CLASSIFIER PREDICTION\n            # GROUND TRUTH GENDER,GENDER DISCRIMINATOR PREDICTION\n            \n            #GROUND TRUTH RATING\n            y_valid_rating_tot = torch.cat((y_valid_rating_tot, y_valid.cpu()))\n            \n            #RATING CLASSIFIER PREDICTION\n            pred_valid_rating_tot = torch.cat((pred_valid_rating_tot, rating_pred_valid.cpu()))\n            \n            #GROUND TRUTH GENDER\n            y_gender_valid_tot = torch.cat((y_gender_valid_tot, y_gender_valid.cpu()))\n            \n            #GENDER DISCRIMINATOR PREDICTION\n            gender_pred_valid_tot = torch.cat((gender_pred_valid_tot, gender_pred_valid.cpu()))\n            \n        ####################### END OF BATCH PROCESSING #######################################\n        \n        \n        ####################### STORING THE RESULTS ##########################################\n        \n        #LOSS PER EPOCH\n        valid_loss_per_epoch.append(sum(total_loss_valid)/len(total_loss_valid))  \n        val_rating_loss_per_epoch.append(sum(total_rating_loss_valid)/len(total_rating_loss_valid))\n        val_gender_loss_per_epoch.append(sum(total_gender_loss_valid)/len(total_gender_loss_valid))\n        \n        #ACCURACY PER EPOCH\n        valid_rating_acc_per_epoch.append(100 * valid_rating_correct / len(valid_data))\n        valid_gender_acc_per_epoch.append(100 * valid_gender_correct / len(valid_data))\n        \n        #WEIGHTED F1 PER EPOCH\n        val_rating_weighted_F1_per_epoch.append(metrics.classification_report(y_valid_rating_tot.cpu(), pred_valid_rating_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        val_gender_weighted_F1_per_epoch.append(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), output_dict=True)['weighted avg']['f1-score'])\n        \n        #CONFUSION MATRIX\n        conf_mat_rating = confusion_matrix(y_valid_rating_tot.cpu(), pred_valid_rating_tot.cpu())\n        conf_mat_gender = confusion_matrix(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu())\n        \n        ##################################### END STORING RESULTS ###############################\n        \n        #################################### PRINTING THE RESULTS ################################\n        \n        #print(\"Validation total_loss = {}\".format(total_valid_loss))\n        \n        # LOSS PER EPOCH - \n        print(\"Valid avg loss for the epoch {} = {}\".format(epoch+1, sum(total_loss_valid)/len(total_loss_valid)))\n        print(\"Valid rating classifier avg loss for the epoch {} = {}\".format(epoch+1, sum(total_rating_loss_valid)/len(total_rating_loss_valid)))\n        print(\"Valid gender discriminator avg loss for the epoch {} = {}\".format(epoch+1, sum(total_gender_loss_valid)/len(total_gender_loss_valid)))\n        \n        avg_loss = total_valid_loss * BATCH_SIZE / len(valid_data)\n        print(\"Validation Avg. Loss: {:.4f}, Rating Accuracy: {:.4f}%\".format(avg_loss, 100 * valid_rating_correct / len(valid_data)))\n        \n        # RATING CLASSIFIER - ACCURACY + F1\n        print(\"Validation total_rating_accuracy = {:.4f}%\".format(100 * valid_rating_correct / len(valid_data)))\n        print(\"Validation total_rating_F1  = {:.4f}%\".format(f1_rating(pred_valid_rating_tot, y_valid_rating_tot)))\n        \n        # GENDER DISCRIMINATOR - ACCURACY + F1\n        print(\"Validation total_gender_accuracy = {:.4f}%\".format(100 * valid_gender_correct / len(valid_data)))\n        print(\"Validation total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_valid_tot,y_gender_valid_tot)))\n        \n        # CONFUSION MATRIX\n        #print(conf_mat_rating)\n        #print(conf_mat_gender)\n        \n        #METRICS CLASSIFICATION REPORT\n        #print(metrics.classification_report(y_valid_rating_tot.cpu(), pred_valid_rating_tot.cpu(), digits=3))\n        #print(metrics.classification_report(y_gender_valid_tot.cpu(), gender_pred_valid_tot.cpu(), digits=3))\n    \n        ################################ END OF PRINT RESULTS #####################################\n        \n        \n    ##################################### END OF VALIDATION AND EPOCH ###############################\n    \n    ############################ STORE CURRENT FOLD RESULTS ################################\n    \n    # CONVERT TO DATAFRAME AND \n    # DOWNLOAD AS CSV - TRAIN RATING ACC, TRAIN RATING F1, VAL RATING ACC, VAL RATING F1\n    # DOWNLOAD AS CSV - TRAIN GENDER DISC ACC, TRAIN GENDER DISC F1, VAL GENDER DISC ACC, VAL GENDER DISC F1\n    # CONSISTING OF ALL 20 EPOCHS RESULT FOR CURRENT FOLD\n    \n    #TRAIN RATING CLASSIFIER ACCURACY\n    train_rating_acc_per_epoch_df = pd.DataFrame(train_rating_acc_per_epoch)\n    train_rating_acc_per_epoch_df.to_csv(\"train_rating_classifier_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #TRAIN RATING CLASSIFIER WEIGHTED F1\n    train_rating_weighted_F1_per_epoch_df = pd.DataFrame(train_rating_weighted_F1_per_epoch)\n    train_rating_weighted_F1_per_epoch_df.to_csv(\"train_rating_classifier_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    #TRAIN GENDER DISCRIMINATOR ACCURACY\n    train_gender_acc_per_epoch_df = pd.DataFrame(train_gender_acc_per_epoch)\n    train_gender_acc_per_epoch_df.to_csv(\"train_gender_disc_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #TRAIN GENDER DISCRIMINATOR WEIGHTED F1\n    train_gender_weighted_F1_per_epoch_df = pd.DataFrame(train_gender_weighted_F1_per_epoch)\n    train_gender_weighted_F1_per_epoch_df.to_csv(\"train_gender_disc_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    #VAL RATING CLASSIFIER ACCURACY\n    valid_rating_acc_per_epoch_df = pd.DataFrame(valid_rating_acc_per_epoch)\n    valid_rating_acc_per_epoch_df.to_csv(\"val_rating_classifier_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #VAL RATING CLASSIFIER WEIGHTED F1\n    val_rating_weighted_F1_per_epoch_df = pd.DataFrame(val_rating_weighted_F1_per_epoch)\n    val_rating_weighted_F1_per_epoch_df.to_csv(\"val_rating_classifier_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    #VAL GENDER DISCRIMINATOR ACCURACY\n    valid_gender_acc_per_epoch_df = pd.DataFrame(valid_gender_acc_per_epoch)\n    valid_gender_acc_per_epoch_df.to_csv(\"val_gender_disc_fold_{}_acc_per_epoch\".format(j), index = False, header = None)\n    \n    #VAL GENDER DISCRIMINATOR WEIGHTED F1\n    val_gender_weighted_F1_per_epoch_df = pd.DataFrame(val_gender_weighted_F1_per_epoch)\n    val_gender_weighted_F1_per_epoch_df.to_csv(\"val_gender_disc_fold_{}_weighted_f1_per_epoch\".format(j), index = False, header = None)\n    \n    ############################# END OF ALL FOLDS PROCESSING ####################################\n    \n    \n#################### CALCULATE THE AVERAGE RESULTS ACROSS FOLDS ###########################################\n\n#GET ACC FOLD RESULTS OF RATING CLASSIFIER\nvf1_rcl_acc = pd.read_csv(val_rating_classifier_fold_1_acc_per_epoch, header = None).to_numpy()\nvf2_rcl_acc = pd.read_csv(val_rating_classifier_fold_2_acc_per_epoch, header = None).to_numpy()\nvf3_rcl_acc = pd.read_csv(val_rating_classifier_fold_3_acc_per_epoch, header = None).to_numpy()\nvf4_rcl_acc = pd.read_csv(val_rating_classifier_fold_4_acc_per_epoch, header = None).to_numpy()\nvf5_rcl_acc = pd.read_csv(val_rating_classifier_fold_5_acc_per_epoch, header = None).to_numpy()\n\n#GET F1 FOLD RESULTS OF RATING CLASSIFIER\nvf1_rcl_f1 = pd.read_csv(val_rating_classifier_fold_1_weighted_f1_per_epoch, header = None).to_numpy()\nvf2_rcl_f1 = pd.read_csv(val_rating_classifier_fold_2_weighted_f1_per_epoch, header = None).to_numpy()\nvf3_rcl_f1 = pd.read_csv(val_rating_classifier_fold_3_weighted_f1_per_epoch, header = None).to_numpy()\nvf4_rcl_f1 = pd.read_csv(val_rating_classifier_fold_4_weighted_f1_per_epoch, header = None).to_numpy()\nvf5_rcl_f1 = pd.read_csv(val_rating_classifier_fold_5_weighted_f1_per_epoch, header = None).to_numpy()\n\n#GET ACC FOLD RESULTS OF GENDER DISCRIMINATOR\nvf1_gd_acc = pd.read_csv(val_gender_disc_fold_1_acc_per_epoch, header = None).to_numpy()\nvf2_gd_acc = pd.read_csv(val_gender_disc_fold_2_acc_per_epoch, header = None).to_numpy()\nvf3_gd_acc = pd.read_csv(val_gender_disc_fold_3_acc_per_epoch, header = None).to_numpy()\nvf4_gd_acc = pd.read_csv(val_gender_disc_fold_4_acc_per_epoch, header = None).to_numpy()\nvf5_gd_acc = pd.read_csv(val_gender_disc_fold_5_acc_per_epoch, header = None).to_numpy()\n\n#GET F1 FOLD RESULTS OF GENDER DISCRIMINATOR\nvf1_gd_f1 = pd.read_csv(val_gender_disc_fold_1_weighted_f1_per_epoch, header = None).to_numpy()\nvf2_gd_f1 = pd.read_csv(val_gender_disc_fold_2_weighted_f1_per_epoch, header = None).to_numpy()\nvf3_gd_f1 = pd.read_csv(val_gender_disc_fold_3_weighted_f1_per_epoch, header = None).to_numpy()\nvf4_gd_f1 = pd.read_csv(val_gender_disc_fold_4_weighted_f1_per_epoch, header = None).to_numpy()\nvf5_gd_f1 = pd.read_csv(val_gender_disc_fold_5_weighted_f1_per_epoch, header = None).to_numpy()\n\n\nepoch_rat_class_acc_avg_for_all_folds = np.zeros((20))\nepoch_rat_class_f1_avg_for_all_folds = np.zeros((20))\nepoch_gender_disc_acc_avg_for_all_folds = np.zeros((20))\nepoch_gender_disc_f1_avg_for_all_folds = np.zeros((20))\n\n#CALCULATE AVG FOLDS RESULTS FOR EACH EPOCH\nfor i in range(20):\n    #RATING CLASSIFIER\n    epoch_rat_class_acc_avg_for_all_folds[i] = (vf1_rcl_acc[i] + vf2_rcl_acc[i] + vf3_rcl_acc[i] + vf4_rcl_acc[i] +vf5_rcl_acc[i])/5\n    epoch_rat_class_f1_avg_for_all_folds[i] = (vf1_rcl_f1[i] + vf2_rcl_f1[i] + vf3_rcl_f1[i] + vf4_rcl_f1[i] +vf5_rcl_f1[i])/5\n    \n    #GENDER DISCRIMINATOR\n    epoch_gender_disc_acc_avg_for_all_folds[i] = (vf1_gd_acc[i] + vf2_gd_acc[i] + vf3_gd_acc[i] + vf4_gd_acc[i] +vf5_gd_acc[i])/5\n    epoch_gender_disc_f1_avg_for_all_folds[i] = (vf1_gd_f1[i] + vf2_gd_f1[i] + vf3_gd_f1[i] + vf4_gd_f1[i] +vf5_gd_f1[i])/5\n\n######################### SAVE AVG FOLDS RESULTS FOR EACH EPOCH ############################\n\n#RATING CLASSIFIER ACCURACY\nepoch_rat_class_acc_avg_for_all_folds_df = pd.DataFrame(epoch_rat_class_acc_avg_for_all_folds)\nepoch_rat_class_acc_avg_for_all_folds_df.to_csv(\"epoch_rat_class_acc_avg_for_all_folds_TowRob.csv\", index = False, header = None)                      \n\n#RATING CLASSIFIER F1\nepoch_rat_class_f1_avg_for_all_folds_df = pd.DataFrame(epoch_rat_class_f1_avg_for_all_folds)\nepoch_rat_class_f1_avg_for_all_folds_df.to_csv(\"epoch_rat_class_f1_avg_for_all_folds_TowRob.csv\", index = False, header = None)\n\n#GENDER DISCRIMINATOR ACCURACY\nepoch_gender_disc_acc_avg_for_all_folds_df = pd.DataFrame(epoch_gender_disc_acc_avg_for_all_folds)\nepoch_gender_disc_acc_avg_for_all_folds_df.to_csv(\"epoch_gender_disc_acc_avg_for_all_folds_TowRob.csv\", index = False, header = None)                      \n\n#GENDER DISCRIMINATOR F1\nepoch_gender_disc_f1_avg_for_all_folds_df = pd.DataFrame(epoch_gender_disc_f1_avg_for_all_folds)\nepoch_gender_disc_f1_avg_for_all_folds_df.to_csv(\"epoch_gender_disc_f1_avg_for_all_folds_TowRob.csv\", index = False, header = None)\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Train and Validation ##########\n\n#Training the entire model together. \n#This was my initial training step, and \n#leads to incorrect training as everything is trained together without adversarial training\n\nfrom torchmetrics import F1Score\ntotal_step = len(train_iter)\n\n\nf1_rating = F1Score(task=\"multiclass\", num_classes=5).to(device)\n#f1_age = F1Score(task=\"multiclass\", num_classes=2).to(device)\nf1_gender = F1Score(task=\"multiclass\", num_classes=2).to(device)\n#f1_location = F1Score(task=\"multiclass\", num_classes=5).to(device)\n\nfor epoch in range(NUM_EPOCHS):\n    \n    y_tot = torch.empty((0))\n    pred_tot = torch.empty((0))\n    y_valid_tot = torch.empty((0))\n    pred_valid_tot = torch.empty((0))\n    y_gender_tot = torch.empty((0))\n    gender_pred_tot = torch.empty((0))\n    y_gender_valid_tot = torch.empty((0))\n    gender_pred_valid_tot = torch.empty((0))\n    \n    \n    model.train()\n    #discriminator_age.train()\n    discriminator_gender.train()\n    #discriminator_location.train()\n\n    total_loss = []\n    train_total_correct = 0\n    #train_age_correct = 0\n    train_gender_correct = 0\n    #train_location_correct = 0\n\n\n    for i, batch in enumerate(train_iter):\n\n        text = batch.text\n        y = batch.rating\n        y_gender = batch.gender\n        #y_age = batch.age\n        #y_location = batch.location\n\n        optimizer_rating.zero_grad()\n        #optimizer_age.zero_grad()\n        optimizer_gender.zero_grad()\n        #optimizer_location.zero_grad()\n\n        # Forward pass\n        # y_pred = model(text).squeeze(1).float()\n        y_pred = model(text).squeeze(1)\n\n        #Changes - This is tricky and I need to understand this or if needed modify this.\n        #### h = model.embedding(text).permute(1, 0, 2)\n\n\n        ## using the correct h\n\n        h = model.hidden_state(text)\n\n        #Changes -  This is where I need to change the input dimensions from h to either c or s, in my work.\n        # Also make sure the training of the discriminators is done correctly first.\n        # Otherwise there will be negative loss.\n        #Also determine which parameters are trained at which step.\n\n\n        #y_age_pred = discriminator_age(h).squeeze()\n        y_gender_pred = discriminator_gender(h).squeeze()\n        #y_location_pred = discriminator_location(h).squeeze()\n\n        rating_loss = criterion_rating(y_pred, y)\n        gender_loss = criterion_gender(y_gender_pred, y_gender)\n        #age_loss = criterion_age(y_age_pred, y_age)\n        #location_loss = criterion_location(y_location_pred, y_location)\n\n        #discriminator_loss = gender_loss + age_loss + location_loss\n        discriminator_loss = gender_loss\n        loss = rating_loss - LAMBDA * discriminator_loss\n\n        rating_pred = torch.argmax(y_pred.data, dim=1)\n        train_total_correct += (rating_pred == y).sum().item()\n\n        #age_pred = torch.argmax(y_age_pred.data, dim=1)\n        #train_age_correct += (age_pred == y_age).sum().item()\n\n        gender_pred = torch.argmax(y_gender_pred.data, dim=1)\n        train_gender_correct += (gender_pred == y_gender).sum().item()\n\n        #location_pred = torch.argmax(y_location_pred.data, dim=1)\n        #train_location_correct += (location_pred == y_location).sum().item()\n\n        # Backward and optimize\n        loss.backward()\n        # discriminator_loss.backward()\n\n        optimizer_rating.step()\n        #optimizer_age.step()\n        optimizer_gender.step()\n        #optimizer_location.step()\n\n        total_loss.append(loss.item())\n        \n        y_tot = torch.cat((y_tot, y.cpu()))\n        pred_tot = torch.cat((pred_tot, rating_pred.cpu()))\n        y_gender_tot = torch.cat((y_gender_tot, y_gender.cpu()))\n        gender_pred_tot = torch.cat((gender_pred_tot, gender_pred.cpu()))\n\n\n        if (i + 1) % 10 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                  .format(epoch + 1, NUM_EPOCHS, i + 1, total_step, loss.item()))\n\n    print(\"Training total_loss = {}\".format(total_loss))\n    print(\"Training total_rating_accuracy = {:.4f}%\".format(100 * train_total_correct / len(train_data)))\n    print(\"Training total_rating_F1  = {:.4f}%\".format(f1_rating(pred_tot, y_tot)))\n    #print(\"Training total_age_accuracy = {:.4f}%\".format(100 * train_age_correct / len(train_data)))\n    #print(\"Training total_age_F1 = {:.4f}%\".format(f1_age(age_pred,y_age)))\n    print(\"Training total_gender_accuracy = {:.4f}%\".format(100 * train_gender_correct / len(train_data)))\n    print(\"Training total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_tot,y_gender_tot)))\n    #print(\"Training total_location_accuracy = {:.4f}%\".format(100 * train_location_correct / len(train_data)))\n    #print(\"Training total_loaction_F1 = {:.4f}%\".format(f1_location(location_pred, y_location)))\n\n    # Validation\n    model.eval()\n    total_valid_correct = 0\n    total_valid_loss = 0.0\n    valid_age_correct = 0\n    valid_gender_correct = 0\n    valid_location_correct = 0\n\n    for i, batch in enumerate(valid_iter):\n        text = batch.text\n        y_valid = batch.rating\n        y_gender_valid = batch.gender\n        #y_age = batch.age\n        #y_location = batch.location\n        \n        if text.size()[0] < 5:\n          continue\n\n        y_pred_valid = model(text).squeeze(1)\n        #h = model.embedding(text).permute(1, 0, 2)\n\n        ## using the correct h\n\n        h = model.hidden_state(text)\n\n        #y_age_pred = discriminator_age(h).squeeze()\n        y_gender_pred_valid = discriminator_gender(h).squeeze()\n        #y_location_pred = discriminator_location(h).squeeze()\n\n        rating_loss = criterion_rating(y_pred, y)\n        gender_loss = criterion_gender(y_gender_pred, y_gender)\n        \n        loss = rating_loss - LAMBDA * gender_loss\n        #age_loss = criterion_age(y_age_pred, y_age)\n        #location_loss = criterion_location(y_location_pred, y_location)\n        # the correct loss is missing.\n        rating_pred_valid = torch.argmax(y_pred_valid.data, dim=1)\n        total_valid_correct += (rating_pred_valid == y_valid).sum().item()\n        total_valid_loss += loss.item()\n\n        #age_pred = torch.argmax(y_age_pred.data, dim=1)\n        #valid_age_correct += (age_pred == y_age).sum().item()\n\n        gender_pred_valid = torch.argmax(y_gender_pred_valid.data, dim=1)\n        valid_gender_correct += (gender_pred_valid == y_gender_valid).sum().item()\n\n        #location_pred = torch.argmax(y_location_pred.data, dim=1)\n        #valid_location_correct += (location_pred == y_location).sum().item()\n        \n        y_valid_tot = torch.cat((y_valid_tot, y_valid.cpu()))\n        pred_valid_tot = torch.cat((pred_valid_tot, rating_pred_valid.cpu()))\n        y_gender_valid_tot = torch.cat((y_gender_valid_tot, y_gender_valid.cpu()))\n        gender_pred_valid_tot = torch.cat((gender_pred_valid_tot, gender_pred_valid.cpu()))\n\n    avg_loss = total_valid_loss * BATCH_SIZE/ len(valid_data)\n    print(\"Validation total_loss = {}\".format(total_valid_loss))\n    print(\n        \"Validation Avg. Loss: {:.4f}, Accuracy: {:.4f}%\".format(avg_loss, 100 * total_valid_correct / len(valid_data)))\n    print(\"Validation total_rating_accuracy = {:.4f}%\".format(100 * total_valid_correct / len(valid_data)))\n    print(\"Validation total_rating_F1  = {:.4f}%\".format(f1_rating(pred_valid_tot, y_valid_tot)))\n    #print(\"Validation total_age_accuracy = {:.4f}%\".format(100 * valid_age_correct / len(valid_data)))\n    #print(\"Validation total_age_F1 = {:.4f}%\".format(f1_age(age_pred,y_age)))\n    print(\"Validation total_gender_accuracy = {:.4f}%\".format(100 * valid_gender_correct / len(valid_data)))\n    print(\"Validation total_gender_F1 = {:.4f}%\".format(f1_gender(gender_pred_valid_tot,y_gender_valid_tot)))\n    #print(\"Validation total_location_accuracy = {:.4f}%\".format(100 * valid_location_correct / len(valid_data)))\n    #print(\"Validation total_loaction_F1 = {:.4f}%\".format(f1_location(location_pred, y_location)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}